# PharmApp Suite
## üß† AI for Drug Discovery and Development üß™
| Copyright 2025 | RnD Pharma Plus | www.nghiencuuthuoc.com | Zalo: +84888999311 |

üß© Topic: Predicting LogP and TPSA of ChEMBL compounds
---
Okay, I understand. You're working with ChEMBL 35 data, using RDKit, and following an AIMLops-inspired directory structure. You're running into issues with your SQL query (regex operator incompatibility) and scikit-learn version. You need assistance in structuring your code within the AIMLops framework, generating SQL and Python code snippets, and addressing the errors you're encountering. And you want to limit the data to 100 rows for faster prototyping.

Here's a breakdown, analysis, code examples, and solutions, formatted for your use case, and presented in both English and Vietnamese.

**1. Analysis of the Analysis Model:**

Your analysis model likely involves extracting bioactivity data from the ChEMBL database, filtering it based on specific criteria (e.g., activity type, standard value), and then using this data in Python with RDKit for tasks such as:

*   **Structure-Activity Relationship (SAR) Modeling:** Building models to predict activity based on molecular features.
*   **Virtual Screening:**  Identifying potential drug candidates by screening a library of compounds against a target.
*   **Lead Optimization:**  Improving the properties of existing drug candidates.
*   **Data Exploration and Visualization:** Understanding trends in the data through plots and statistical analysis.

The SQL code serves to extract and prepare the data.  The Python code uses RDKit for molecular manipulation and potentially scikit-learn for machine learning models.

**Vietnamese Translation:**

*M√¥ h√¨nh ph√¢n t√≠ch c·ªßa b·∫°n c√≥ kh·∫£ nƒÉng li√™n quan ƒë·∫øn vi·ªác tr√≠ch xu·∫•t d·ªØ li·ªáu ho·∫°t t√≠nh sinh h·ªçc t·ª´ c∆° s·ªü d·ªØ li·ªáu ChEMBL, l·ªçc d·ªØ li·ªáu d·ª±a tr√™n c√°c ti√™u ch√≠ c·ª• th·ªÉ (v√≠ d·ª•: lo·∫°i ho·∫°t ƒë·ªông, gi√° tr·ªã ti√™u chu·∫©n), sau ƒë√≥ s·ª≠ d·ª•ng d·ªØ li·ªáu n√†y trong Python v·ªõi RDKit cho c√°c t√°c v·ª• nh∆∞:*

*   *M√¥ h√¨nh h√≥a m·ªëi quan h·ªá c·∫•u tr√∫c-ho·∫°t t√≠nh (SAR): X√¢y d·ª±ng m√¥ h√¨nh ƒë·ªÉ d·ª± ƒëo√°n ho·∫°t ƒë·ªông d·ª±a tr√™n c√°c ƒë·∫∑c ƒëi·ªÉm ph√¢n t·ª≠.*
*   *S√†ng l·ªçc ·∫£o: X√°c ƒë·ªãnh c√°c ·ª©ng c·ª≠ vi√™n thu·ªëc ti·ªÅm nƒÉng b·∫±ng c√°ch s√†ng l·ªçc m·ªôt th∆∞ vi·ªán c√°c h·ª£p ch·∫•t ƒë·ªëi v·ªõi m·ªôt m·ª•c ti√™u.*
*   *T·ªëi ∆∞u h√≥a d·∫´n ƒë·∫ßu: C·∫£i thi·ªán c√°c thu·ªôc t√≠nh c·ªßa c√°c ·ª©ng c·ª≠ vi√™n thu·ªëc hi·ªán c√≥.*
*   *Kh√°m ph√° v√† tr·ª±c quan h√≥a d·ªØ li·ªáu: Hi·ªÉu c√°c xu h∆∞·ªõng trong d·ªØ li·ªáu th√¥ng qua c√°c bi·ªÉu ƒë·ªì v√† ph√¢n t√≠ch th·ªëng k√™.*

*M√£ SQL ph·ª•c v·ª• ƒë·ªÉ tr√≠ch xu·∫•t v√† chu·∫©n b·ªã d·ªØ li·ªáu. M√£ Python s·ª≠ d·ª•ng RDKit ƒë·ªÉ thao t√°c ph√¢n t·ª≠ v√† c√≥ kh·∫£ nƒÉng s·ª≠ d·ª•ng scikit-learn cho c√°c m√¥ h√¨nh h·ªçc m√°y.*

**2. AIMLops Directory Structure and Code Integration**

Assuming a simplified AIMLops-inspired structure, it might look like this:

```
Topic_CheMBL_35_11/
‚îú‚îÄ‚îÄ data/
‚îÇ   ‚îî‚îÄ‚îÄ chembl_activity_data.csv
‚îú‚îÄ‚îÄ notebooks/
‚îÇ   ‚îî‚îÄ‚îÄ Topic_CheMBL_35_11_1_data_extraction_and_cleaning.ipynb
‚îÇ   ‚îî‚îÄ‚îÄ Topic_CheMBL_35_11_2_sar_analysis.ipynb
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îî‚îÄ‚îÄ utils.py  # Optional:  Reusable functions
‚îî‚îÄ‚îÄ README.md
```

*   **`data/`**:  Stores your extracted data (e.g., `chembl_activity_data.csv`).
*   **`notebooks/`**: Contains your Jupyter notebooks.
*   **`src/`**: (Optional) Holds Python modules with reusable functions.

**Vietnamese Translation:**

*Gi·∫£ s·ª≠ m·ªôt c·∫•u tr√∫c l·∫•y c·∫£m h·ª©ng t·ª´ AIMLops ƒë∆°n gi·∫£n, n√≥ c√≥ th·ªÉ tr√¥ng nh∆∞ th·∫ø n√†y:*

```
Topic_CheMBL_35_11/
‚îú‚îÄ‚îÄ data/
‚îÇ   ‚îî‚îÄ‚îÄ chembl_activity_data.csv
‚îú‚îÄ‚îÄ notebooks/
‚îÇ   ‚îî‚îÄ‚îÄ Topic_CheMBL_35_11_1_data_extraction_and_cleaning.ipynb
‚îÇ   ‚îî‚îÄ‚îÄ Topic_CheMBL_35_11_2_phan_tich_sar.ipynb
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îî‚îÄ‚îÄ utils.py  # T√πy ch·ªçn: C√°c h√†m c√≥ th·ªÉ t√°i s·ª≠ d·ª•ng
‚îî‚îÄ‚îÄ README.md
```

*   *`data/`*: L∆∞u tr·ªØ d·ªØ li·ªáu ƒë√£ tr√≠ch xu·∫•t c·ªßa b·∫°n (v√≠ d·ª•: `chembl_activity_data.csv`).
*   *`notebooks/`*: Ch·ª©a c√°c s·ªï tay Jupyter c·ªßa b·∫°n.
*   *`src/`*: (T√πy ch·ªçn) Ch·ª©a c√°c m√¥-ƒëun Python v·ªõi c√°c h√†m c√≥ th·ªÉ t√°i s·ª≠ d·ª•ng.

**3. SQL Code (Addressing Regex Issue and Limiting to 100 Rows)**

The error "ERROR: operator does not exist: numeric ~ unknown, LINE 12: AND act.standard_value ~ '^[0-9\.]+$'" indicates that the `~` operator (PostgreSQL's regular expression match) is not compatible with the `numeric` data type in the `standard_value` column.  We need to cast the column to text *before* applying the regex. Also, the `LIMIT 100` clause will restrict the output to 100 rows.

```sql
-- File: data/chembl_activity_data.sql
SELECT
    act.activity_id,
    cmp.chembl_id,
    act.standard_type,
    act.standard_value,
    act.standard_units,
    mol.molfile
FROM activities act
JOIN assays ass ON act.assay_id = ass.assay_id
JOIN target_dictionary td ON ass.tid = td.tid
JOIN molecule_dictionary md ON act.molregno = md.molregno
JOIN compound_structures cmp ON md.molregno = cmp.molregno
LEFT JOIN mols mol ON md.molregno = mol.molregno
WHERE td.target_type = 'SINGLE PROTEIN'
  AND ass.assay_type = 'B'
  AND act.standard_type = 'IC50'
  AND act.standard_units = 'nM'
  AND act.standard_value IS NOT NULL
  AND act.standard_value::TEXT ~ '^[0-9.]+$' -- Cast to TEXT before regex
LIMIT 100;
```

**Explanation:**

*   `act.standard_value::TEXT ~ '^[0-9.]+$'` : This line explicitly casts the `standard_value` column to the `TEXT` data type before applying the regular expression.  `^[0-9.]+$` ensures that the value consists only of digits and periods.
*   `LIMIT 100`: Restricts the number of rows returned to 100.

**How to run:**

1.  Open pgAdmin and connect to your `chembl_35` database.
2.  Open a new query window.
3.  Paste the SQL code above into the query window.
4.  Execute the query.
5.  **Important:** In pgAdmin, use the "Copy with Headers" option after running the query to copy the result to your clipboard. Then, save this clipboard content as a CSV file named `chembl_activity_data.csv` inside your `data/` directory.  Alternatively, explore pgAdmin's export functionality for more robust CSV creation.

**Vietnamese Translation:**

*   *`act.standard_value::TEXT ~ '^[0-9.]+$'` : D√≤ng n√†y chuy·ªÉn ƒë·ªïi r√µ r√†ng c·ªôt `standard_value` th√†nh ki·ªÉu d·ªØ li·ªáu `TEXT` tr∆∞·ªõc khi √°p d·ª•ng bi·ªÉu th·ª©c ch√≠nh quy. `^[0-9.]+$` ƒë·∫£m b·∫£o r·∫±ng gi√° tr·ªã ch·ªâ bao g·ªìm c√°c ch·ªØ s·ªë v√† d·∫•u ch·∫•m.*
*   `LIMIT 100`: H·∫°n ch·∫ø s·ªë l∆∞·ª£ng h√†ng tr·∫£ v·ªÅ l√† 100.

**H∆∞·ªõng d·∫´n ch·∫°y:**

1.  M·ªü pgAdmin v√† k·∫øt n·ªëi v·ªõi c∆° s·ªü d·ªØ li·ªáu `chembl_35` c·ªßa b·∫°n.
2.  M·ªü m·ªôt c·ª≠a s·ªï truy v·∫•n m·ªõi.
3.  D√°n m√£ SQL ·ªü tr√™n v√†o c·ª≠a s·ªï truy v·∫•n.
4.  Th·ª±c thi truy v·∫•n.
5.  **Quan tr·ªçng:** Trong pgAdmin, s·ª≠ d·ª•ng t√πy ch·ªçn "Sao ch√©p v·ªõi ti√™u ƒë·ªÅ" sau khi ch·∫°y truy v·∫•n ƒë·ªÉ sao ch√©p k·∫øt qu·∫£ v√†o clipboard c·ªßa b·∫°n. Sau ƒë√≥, l∆∞u n·ªôi dung clipboard n√†y d∆∞·ªõi d·∫°ng t·ªáp CSV c√≥ t√™n `chembl_activity_data.csv` b√™n trong th∆∞ m·ª•c `data/` c·ªßa b·∫°n. Ngo√†i ra, h√£y kh√°m ph√° ch·ª©c nƒÉng xu·∫•t c·ªßa pgAdmin ƒë·ªÉ t·∫°o CSV m·∫°nh m·∫Ω h∆°n.

**4. Python Code (Example Notebooks)**

Here are two example notebooks to demonstrate how to read the data, use RDKit, and perform some basic analysis.

**Notebook 1: `notebooks/Topic_CheMBL_35_11_1_data_extraction_and_cleaning.ipynb`**

```python
# ---
# jupyter:
#   jupytext:
#     text_representation:
#       extension: .py
#       format_name: percent
#       format_version: '1.3'
#       jupytext_version: 1.14.5
#   kernelspec:
#     display_name: Python 3 (ipykernel)
#     language: python
#     name: python3
# ---

# %% [markdown]
# # Topic_CheMBL_35_11_1: Data Extraction and Cleaning

# %%
import pandas as pd
import os
from rdkit import Chem

# Define the base path
base_path = os.path.dirname(os.getcwd())
data_path = os.path.join(base_path, 'data', 'chembl_activity_data.csv')

# %%
# Load the data
try:
    df = pd.read_csv(data_path)
except FileNotFoundError:
    print(f"Error: File not found at {data_path}. Make sure the CSV file exists.")
    exit()

# %%
# Display the first few rows
print(df.head())

# %%
# Check for missing values
print("\nMissing Values:")
print(df.isnull().sum())

# %%
# Basic data cleaning: Drop rows with missing 'molfile'
df = df.dropna(subset=['molfile'])

# %%
# Convert molfile to RDKit Mol object
def molfile_to_mol(molfile):
    try:
        mol = Chem.MolFromMolBlock(molfile)
        if mol is None:
            return None
        return mol
    except:
        return None

df['mol'] = df['molfile'].apply(molfile_to_mol)

# Drop rows where the conversion to Mol object failed
df = df.dropna(subset=['mol'])

# Display the cleaned data
print("\nCleaned Data:")
print(df.head())

# %%
#Basic data cleaning: keep the IC50 data type.
df = df[df['standard_type'] == 'IC50']

# %%
# Save the cleaned data (optional)
cleaned_data_path = os.path.join(base_path, 'data', 'chembl_activity_data_cleaned.csv')
df.to_csv(cleaned_data_path, index=False)
print(f"\nCleaned data saved to {cleaned_data_path}")
```

**Notebook 2: `notebooks/Topic_CheMBL_35_11_2_sar_analysis.ipynb`**

```python
# ---
# jupyter:
#   jupytext:
#     text_representation:
#       extension: .py
#       format_name: percent
#       format_version: '1.3'
#       jupytext_version: 1.14.5
#   kernelspec:
#     display_name: Python 3 (ipykernel)
#     language: python
#     name: python3
# ---

# %% [markdown]
# # Topic_CheMBL_35_11_2: SAR Analysis

# %%
import pandas as pd
import os
from rdkit import Chem
from rdkit.Chem import Descriptors
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score
import matplotlib.pyplot as plt

# Define the base path
base_path = os.path.dirname(os.getcwd())
cleaned_data_path = os.path.join(base_path, 'data', 'chembl_activity_data_cleaned.csv')

# %%
# Load the cleaned data
try:
    df = pd.read_csv(cleaned_data_path)
except FileNotFoundError:
    print(f"Error: File not found at {cleaned_data_path}.  Make sure you run the first notebook first.")
    exit()

# Convert molfile to RDKit Mol object
def molfile_to_mol(molfile):
    try:
        mol = Chem.MolFromMolBlock(molfile)
        if mol is None:
            return None
        return mol
    except:
        return None

df['mol'] = df['molfile'].apply(molfile_to_mol)

# Drop rows where the conversion to Mol object failed
df = df.dropna(subset=['mol'])

# %%
# Calculate molecular weight
def calculate_mw(mol):
    return Descriptors.MolWt(mol)

df['mol_weight'] = df['mol'].apply(calculate_mw)

# %%
# Prepare data for modeling
X = df[['mol_weight']]  # Feature: Molecular Weight
y = -np.log10(df['standard_value'])  # Target: pIC50 (transformed IC50)

# Split data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# %%
# Train a linear regression model
model = LinearRegression()
model.fit(X_train, y_train)

# %%
# Make predictions
y_pred = model.predict(X_test)

# %%
# Evaluate the model
mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)

print(f"Mean Squared Error: {mse}")
print(f"R-squared: {r2}")

# %%
# Plot predictions vs. actual values
plt.scatter(y_test, y_pred)
plt.xlabel("Actual pIC50")
plt.ylabel("Predicted pIC50")
plt.title("Actual vs. Predicted pIC50")
plt.show()
```

**Explanation:**

*   **`os.path.join(base_path, ...)`**:  This ensures your code works regardless of where the notebook is run, as long as the directory structure is maintained.
*   **RDKit `Chem.MolFromMolBlock(molfile)`**: This converts the `molfile` string (representing the molecular structure) into an RDKit `Mol` object, which can then be used for calculations.
*   **`Descriptors.MolWt(mol)`**: This calculates the molecular weight of the molecule.
*   **`-np.log10(df['standard_value'])`**:  This transforms the IC50 value to pIC50 (a logarithmic scale), which is often a better representation for SAR modeling.
*   **`train_test_split`**, **`LinearRegression`**, **`mean_squared_error`**, **`r2_score`**: These are scikit-learn functions for splitting data, training a linear regression model, and evaluating its performance.  The `squared=False` parameter is *not* needed for `mean_squared_error` in current versions of scikit-learn.

**Vietnamese Translation:**

*   *`os.path.join(base_path, ...)`*: ƒêi·ªÅu n√†y ƒë·∫£m b·∫£o m√£ c·ªßa b·∫°n ho·∫°t ƒë·ªông b·∫•t k·ªÉ s·ªï tay ƒë∆∞·ª£c ch·∫°y ·ªü ƒë√¢u, mi·ªÖn l√† c·∫•u tr√∫c th∆∞ m·ª•c ƒë∆∞·ª£c duy tr√¨.
*   *RDKit `Chem.MolFromMolBlock(molfile)`*: ƒêi·ªÅu n√†y chuy·ªÉn ƒë·ªïi chu·ªói `molfile` (ƒë·∫°i di·ªán cho c·∫•u tr√∫c ph√¢n t·ª≠) th√†nh ƒë·ªëi t∆∞·ª£ng `Mol` c·ªßa RDKit, sau ƒë√≥ c√≥ th·ªÉ ƒë∆∞·ª£c s·ª≠ d·ª•ng cho c√°c ph√©p t√≠nh.
*   *`Descriptors.MolWt(mol)`*: ƒêi·ªÅu n√†y t√≠nh to√°n tr·ªçng l∆∞·ª£ng ph√¢n t·ª≠ c·ªßa ph√¢n t·ª≠.
*   *`-np.log10(df['standard_value'])`*: ƒêi·ªÅu n√†y chuy·ªÉn ƒë·ªïi gi√° tr·ªã IC50 th√†nh pIC50 (thang logarit), th∆∞·ªùng l√† m·ªôt bi·ªÉu di·ªÖn t·ªët h∆°n cho m√¥ h√¨nh h√≥a SAR.
*   *`train_test_split`*, *`LinearRegression`*, *`mean_squared_error`*, *`r2_score`*: ƒê√¢y l√† c√°c h√†m scikit-learn ƒë·ªÉ chia d·ªØ li·ªáu, hu·∫•n luy·ªán m√¥ h√¨nh h·ªìi quy tuy·∫øn t√≠nh v√† ƒë√°nh gi√° hi·ªáu su·∫•t c·ªßa n√≥. Tham s·ªë `squared=False` *kh√¥ng* c·∫ßn thi·∫øt cho `mean_squared_error` trong c√°c phi√™n b·∫£n scikit-learn hi·ªán t·∫°i.

**5. Addressing Errors**

*   **`ERROR: operator does not exist: numeric ~ unknown`**:  Add `::TEXT` to cast the numeric column to text before applying the regex (as shown in the SQL code above).
*   **`old scikit-learn version does not support parameters squared=False in the mean_squared_error function`**:  Remove the `squared=False` parameter from the `mean_squared_error` function.  The default behavior is to return the mean squared error.  If you *need* the root mean squared error, calculate it manually: `rmse = np.sqrt(mean_squared_error(y_test, y_pred))`

**Vietnamese Translation:**

*   *`ERROR: operator does not exist: numeric ~ unknown`*: Th√™m `::TEXT` ƒë·ªÉ chuy·ªÉn ƒë·ªïi c·ªôt s·ªë th√†nh vƒÉn b·∫£n tr∆∞·ªõc khi √°p d·ª•ng regex (nh∆∞ ƒë∆∞·ª£c hi·ªÉn th·ªã trong m√£ SQL ·ªü tr√™n).
*   *`old scikit-learn version does not support parameters squared=False in the mean_squared_error function`*: X√≥a tham s·ªë `squared=False` kh·ªèi h√†m `mean_squared_error`. H√†nh vi m·∫∑c ƒë·ªãnh l√† tr·∫£ v·ªÅ l·ªói b√¨nh ph∆∞∆°ng trung b√¨nh. N·∫øu b·∫°n *c·∫ßn* cƒÉn b·∫≠c hai c·ªßa l·ªói b√¨nh ph∆∞∆°ng trung b√¨nh, h√£y t√≠nh n√≥ th·ªß c√¥ng: `rmse = np.sqrt(mean_squared_error(y_test, y_pred))`

**6. Five Code Examples (Extending the Analysis)**

Here are five additional code examples to extend your analysis, focusing on different aspects of the data and RDKit:

**Example 1: Calculating LogP**

```python
# In notebooks/Topic_CheMBL_35_11_2_sar_analysis.ipynb (or a new notebook)
from rdkit.Chem import AllChem
from rdkit.Chem import Crippen

def calculate_logp(mol):
    return Crippen.MolLogP(mol)

df['logp'] = df['mol'].apply(calculate_logp)
print(df[['chembl_id', 'logp']].head())
```

**Example 2: Calculating TPSA (Topological Polar Surface Area)**

```python
# In notebooks/Topic_CheMBL_35_11_2_sar_analysis.ipynb (or a new notebook)
from rdkit.Chem import Descriptors3D

def calculate_tpsa(mol):
    return Descriptors3D.TPSA(mol)

df['tpsa'] = df['mol'].apply(calculate_tpsa)
print(df[['chembl_id', 'tpsa']].head())
```

**Example 3: Creating a Histogram of Molecular Weights**

```python
# In notebooks/Topic_CheMBL_35_11_2_sar_analysis.ipynb (or a new notebook)
import matplotlib.pyplot as plt

plt.hist(df['mol_weight'], bins=20)
plt.xlabel("Molecular Weight")
plt.ylabel("Frequency")
plt.title("Distribution of Molecular Weights")
plt.show()
```

**Example 4: Calculating Morgan Fingerprints (ECFP4) and PCA (for visualization)**

```python
# In notebooks/Topic_CheMBL_35_11_2_sar_analysis.ipynb (or a new notebook)
from rdkit.Chem import AllChem
from sklearn.decomposition import PCA

def calculate_ecfp4(mol):
    return AllChem.GetMorganFingerprintAsBitVect(mol, 2, nBits=1024)

df['ecfp4'] = df['mol'].apply(calculate_ecfp4)

# Convert fingerprints to a matrix
X = np.array([list(fp) for fp in df['ecfp4']])

# Perform PCA for dimensionality reduction
pca = PCA(n_components=2)
pca.fit(X)
X_pca = pca.transform(X)

# Plot the PCA results
plt.scatter(X_pca[:, 0], X_pca[:, 1])
plt.xlabel("PCA Component 1")
plt.ylabel("PCA Component 2")
plt.title("PCA of ECFP4 Fingerprints")
plt.show()
```

**Example 5: Building a slightly more complex model (using multiple features). Requires the cleaned data from the first notebook.**

```python
# In notebooks/Topic_CheMBL_35_11_2_sar_analysis.ipynb (or a new notebook)

from rdkit.Chem import AllChem
from rdkit.Chem import Crippen
from rdkit.Chem import Descriptors3D
import numpy as np
import pandas as pd
import os
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score
import matplotlib.pyplot as plt
from rdkit import Chem

# Define the base path
base_path = os.path.dirname(os.getcwd())
cleaned_data_path = os.path.join(base_path, 'data', 'chembl_activity_data_cleaned.csv')

# Load the cleaned data
try:
    df = pd.read_csv(cleaned_data_path)
except FileNotFoundError:
    print(f"Error: File not found at {cleaned_data_path}.  Make sure you run the first notebook first.")
    exit()

# Convert molfile to RDKit Mol object
def molfile_to_mol(molfile):
    try:
        mol = Chem.MolFromMolBlock(molfile)
        if mol is None:
            return None
        return mol
    except:
        return None

df['mol'] = df['molfile'].apply(molfile_to_mol)

# Drop rows where the conversion to Mol object failed
df = df.dropna(subset=['mol'])
# Ensure we calculate the features after handling missing MOL objects.
def calculate_mw(mol):
    return Descriptors.MolWt(mol)

def calculate_logp(mol):
    return Crippen.MolLogP(mol)

def calculate_tpsa(mol):
    return Descriptors3D.TPSA(mol)


df['mol_weight'] = df['mol'].apply(calculate_mw)
df['logp'] = df['mol'].apply(calculate_logp)
df['tpsa'] = df['mol'].apply(calculate_tpsa)

# Prepare data for modeling.  Handle any newly introduced NaNs from descriptor calculation
df = df.dropna(subset=['mol_weight', 'logp', 'tpsa'])

X = df[['mol_weight', 'logp', 'tpsa']]  # Features: Molecular Weight, LogP, TPSA
y = -np.log10(df['standard_value'])  # Target: pIC50 (transformed IC50)

# Split data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Train a linear regression model
model = LinearRegression()
model.fit(X_train, y_train)

# Make predictions
y_pred = model.predict(X_test)

# Evaluate the model
mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)

print(f"Mean Squared Error: {mse}")
print(f"R-squared: {r2}")

# Plot predictions vs. actual values
plt.scatter(y_test, y_pred)
plt.xlabel("Actual pIC50")
plt.ylabel("Predicted pIC50")
plt.title("Actual vs. Predicted pIC50")
plt.show()
```

**Vietnamese Translation (for Example 5 - the most complex one):**

```python
# Trong notebooks/Topic_CheMBL_35_11_2_phan_tich_sar.ipynb (ho·∫∑c m·ªôt s·ªï tay m·ªõi)

from rdkit.Chem import AllChem
from rdkit.Chem import Crippen
from rdkit.Chem import Descriptors3D
import numpy as np
import pandas as pd
import os
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score
import matplotlib.pyplot as plt
from rdkit import Chem

# X√°c ƒë·ªãnh ƒë∆∞·ªùng d·∫´n c∆° s·ªü
base_path = os.path.dirname(os.getcwd())
cleaned_data_path = os.path.join(base_path, 'data', 'chembl_activity_data_cleaned.csv')

# T·∫£i d·ªØ li·ªáu ƒë√£ l√†m s·∫°ch
try:
    df = pd.read_csv(cleaned_data_path)
except FileNotFoundError:
    print(f"L·ªói: Kh√¥ng t√¨m th·∫•y t·ªáp t·∫°i {cleaned_data_path}.  ƒê·∫£m b·∫£o b·∫°n ƒë√£ ch·∫°y s·ªï tay ƒë·∫ßu ti√™n tr∆∞·ªõc.")
    exit()

# Chuy·ªÉn ƒë·ªïi molfile th√†nh ƒë·ªëi t∆∞·ª£ng Mol c·ªßa RDKit
def molfile_to_mol(molfile):
    try:
        mol = Chem.MolFromMolBlock(molfile)
        if mol is None:
            return None
        return mol
    except:
        return None

df['mol'] = df['molfile'].apply(molfile_to_mol)

# Lo·∫°i b·ªè c√°c h√†ng m√† vi·ªác chuy·ªÉn ƒë·ªïi sang ƒë·ªëi t∆∞·ª£ng Mol kh√¥ng th√†nh c√¥ng
df = df.dropna(subset=['mol'])
# ƒê·∫£m b·∫£o ch√∫ng ta t√≠nh to√°n c√°c ƒë·∫∑c tr∆∞ng sau khi x·ª≠ l√Ω c√°c ƒë·ªëi t∆∞·ª£ng MOL b·ªã thi·∫øu.
def calculate_mw(mol):
    return Descriptors.MolWt(mol)

def calculate_logp(mol):
    return Crippen.MolLogP(mol)

def calculate_tpsa(mol):
    return Descriptors3D.TPSA(mol)


df['mol_weight'] = df['mol'].apply(calculate_mw)
df['logp'] = df['mol'].apply(calculate_logp)
df['tpsa'] = df['mol'].apply(calculate_tpsa)

# Chu·∫©n b·ªã d·ªØ li·ªáu ƒë·ªÉ m√¥ h√¨nh h√≥a. X·ª≠ l√Ω b·∫•t k·ª≥ NaN m·ªõi n√†o ƒë∆∞·ª£c ƒë∆∞a v√†o t·ª´ vi·ªác t√≠nh to√°n descriptor
df = df.dropna(subset=['mol_weight', 'logp', 'tpsa'])

X = df[['mol_weight', 'logp', 'tpsa']]  # C√°c ƒë·∫∑c tr∆∞ng: Tr·ªçng l∆∞·ª£ng ph√¢n t·ª≠, LogP, TPSA
y = -np.log10(df['standard_value'])  # M·ª•c ti√™u: pIC50 (IC50 ƒë√£ chuy·ªÉn ƒë·ªïi)

# Chia d·ªØ li·ªáu th√†nh t·∫≠p hu·∫•n luy·ªán v√† t·∫≠p ki·ªÉm tra
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Hu·∫•n luy·ªán m·ªôt m√¥ h√¨nh h·ªìi quy tuy·∫øn t√≠nh
model = LinearRegression()
model.fit(X_train, y_train)

# D·ª± ƒëo√°n
y_pred = model.predict(X_test)

# ƒê√°nh gi√° m√¥ h√¨nh
mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)

print(f"L·ªói b√¨nh ph∆∞∆°ng trung b√¨nh: {mse}")
print(f"R-squared: {r2}")

# V·∫Ω ƒë·ªì th·ªã d·ª± ƒëo√°n so v·ªõi c√°c gi√° tr·ªã th·ª±c t·∫ø
plt.scatter(y_test, y_pred)
plt.xlabel("pIC50 th·ª±c t·∫ø")
plt.ylabel("pIC50 d·ª± ƒëo√°n")
plt.title("pIC50 th·ª±c t·∫ø so v·ªõi d·ª± ƒëo√°n")
plt.show()
```

**Key Improvements and Considerations:**

*   **Error Handling:**  The Python code includes `try...except` blocks for file loading and molecule conversion to gracefully handle potential errors.
*   **Data Cleaning:**  The code explicitly handles missing values (`NaNs`) by dropping rows with missing data.  This is *crucial* for many RDKit calculations and scikit-learn models.
*   **Feature Engineering:** The examples demonstrate calculating molecular weight, LogP, TPSA, and ECFP4 fingerprints, which are commonly used features in SAR models.
*   **Model Evaluation:**  The code calculates the Mean Squared Error (MSE) and R-squared (R¬≤) to evaluate the performance of the linear regression model.
*   **Visualization:**  The examples include plots to visualize the distribution of molecular weights and the results of the PCA analysis.
*   **Flexibility:**  The code is designed to be modular and easily adaptable to different datasets and analysis tasks.  You can add more features, try different machine learning models, and customize the visualizations.
*   **Directory Structure:** The code adheres to the AIMLops-inspired directory structure, making it easier to manage and deploy your project.
*   **Upgrading scikit-learn:** It's strongly recommended to update to a recent version of scikit-learn to avoid compatibility issues.  You can do this using `pip install --upgrade scikit-learn`.
*   **Data Exploration:**  Before building complex models, always explore your data thoroughly.  Look for outliers, missing values, and potential biases.

This comprehensive response provides a solid foundation for your ChEMBL 35 data analysis project. Remember to adapt the code and analysis to your specific research questions and goals. Good luck!
