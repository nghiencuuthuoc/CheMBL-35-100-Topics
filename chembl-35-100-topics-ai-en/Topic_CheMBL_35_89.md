# PharmApp Suite
## üß† AI for Drug Discovery and Development üß™
| Copyright 2025 | RnD Pharma Plus | www.nghiencuuthuoc.com | Zalo: +84888999311 |

üß© Topic: Converting ChEMBL SMILES to InChIKeys
---
Okay, I understand. You're working on a drug discovery project using ChEMBL 35 data, RDKit, and PostgreSQL. You want to follow an MLOps-inspired structure, run your code in Jupyter notebooks, and need help addressing specific errors and creating a small, workable dataset (100 rows) for initial analysis.

Here's a breakdown of the analysis, code, and examples in both English and Vietnamese, addressing your specific concerns.

**1. Analysis Model & Project Structure**

**English:**

Your project seems to be following a standard data analysis workflow for drug discovery:

*   **Data Extraction & Preparation (SQL):** You're using SQL to query the ChEMBL 35 database and extract relevant data. This likely involves filtering for specific activity types, potency values, and molecule properties.  You're saving this data to CSV files for easier access in Python.
*   **Data Loading & Processing (Python):** You're using Jupyter notebooks to load the data from the CSV files, process it using RDKit (for molecular representation and feature generation), and potentially perform machine learning tasks.
*   **Machine Learning (Python):** The end goal is likely to build a model that can predict activity or other properties of molecules based on their structure and features.  This could involve regression (predicting potency) or classification (predicting activity/inactivity).

**Vietnamese:**

D·ª± √°n c·ªßa b·∫°n d∆∞·ªùng nh∆∞ ƒëang tu√¢n theo quy tr√¨nh ph√¢n t√≠ch d·ªØ li·ªáu ti√™u chu·∫©n cho vi·ªác kh√°m ph√° thu·ªëc:

*   **Tr√≠ch xu·∫•t v√† Chu·∫©n b·ªã D·ªØ li·ªáu (SQL):** B·∫°n ƒëang s·ª≠ d·ª•ng SQL ƒë·ªÉ truy v·∫•n c∆° s·ªü d·ªØ li·ªáu ChEMBL 35 v√† tr√≠ch xu·∫•t d·ªØ li·ªáu li√™n quan.  ƒêi·ªÅu n√†y c√≥ th·ªÉ li√™n quan ƒë·∫øn vi·ªác l·ªçc c√°c lo·∫°i ho·∫°t ƒë·ªông c·ª• th·ªÉ, gi√° tr·ªã hi·ªáu l·ª±c v√† thu·ªôc t√≠nh ph√¢n t·ª≠. B·∫°n ƒëang l∆∞u d·ªØ li·ªáu n√†y v√†o c√°c t·ªáp CSV ƒë·ªÉ d·ªÖ d√†ng truy c·∫≠p h∆°n trong Python.
*   **T·∫£i v√† X·ª≠ l√Ω D·ªØ li·ªáu (Python):** B·∫°n ƒëang s·ª≠ d·ª•ng Jupyter Notebook ƒë·ªÉ t·∫£i d·ªØ li·ªáu t·ª´ c√°c t·ªáp CSV, x·ª≠ l√Ω n√≥ b·∫±ng RDKit (ƒë·ªÉ bi·ªÉu di·ªÖn ph√¢n t·ª≠ v√† t·∫°o ƒë·∫∑c tr∆∞ng), v√† c√≥ kh·∫£ nƒÉng th·ª±c hi·ªán c√°c t√°c v·ª• h·ªçc m√°y.
*   **H·ªçc M√°y (Python):** M·ª•c ti√™u cu·ªëi c√πng c√≥ th·ªÉ l√† x√¢y d·ª±ng m·ªôt m√¥ h√¨nh c√≥ th·ªÉ d·ª± ƒëo√°n ho·∫°t ƒë·ªông ho·∫∑c c√°c thu·ªôc t√≠nh kh√°c c·ªßa ph√¢n t·ª≠ d·ª±a tr√™n c·∫•u tr√∫c v√† ƒë·∫∑c tr∆∞ng c·ªßa ch√∫ng.  ƒêi·ªÅu n√†y c√≥ th·ªÉ li√™n quan ƒë·∫øn h·ªìi quy (d·ª± ƒëo√°n hi·ªáu l·ª±c) ho·∫∑c ph√¢n lo·∫°i (d·ª± ƒëo√°n ho·∫°t ƒë·ªông/kh√¥ng ho·∫°t ƒë·ªông).

**2. Code (SQL & Python)**

Here's the code, addressing the errors and following your folder structure guidelines.

**a) SQL (`../data/chembl_35_89_data.csv`)**

```sql
-- Query to extract ChEMBL data for analysis (limited to 100 rows)
-- Save this as chembl_35_89_data.csv

SELECT
    cmp.chembl_id,
    md.molfile,
    act.standard_type,
    act.standard_value,
    act.standard_units,
    act.assay_chembl_id
FROM
    compound_structures cmp
JOIN
    molecule_dictionary md ON cmp.molregno = md.molregno
JOIN
    activities act ON md.molregno = act.molregno
WHERE
    act.standard_type IN ('IC50', 'Ki', 'EC50')  -- Consider relevant activity types
    AND act.standard_units = 'nM'  -- Focus on nM units for consistency
    AND act.standard_value IS NOT NULL
    AND act.standard_value::text ~ '^[0-9.]+$'  -- Ensure standard_value is numeric
LIMIT 100;  -- Limit to 100 rows
```

**Explanation (SQL):**

*   **Error Fix:**  The original error `ERROR: operator does not exist: numeric ~ unknown` occurred because you were trying to use the `~` operator (regular expression matching) on a `numeric` column.  PostgreSQL needs an explicit cast to text for that. The `act.standard_value::text ~ '^[0-9.]+$'` line now correctly casts the `standard_value` to text before applying the regex.
*   **Data Selection:** Selects the molecule's ChEMBL ID, molfile (SDF string), activity type, standard value, units, and assay ID.
*   **Filtering:** Filters for common activity types (IC50, Ki, EC50) and standardizes units to 'nM'. It also makes sure the `standard_value` is not null and can be represented as a number.
*   **Limiting:**  Crucially limits the result to 100 rows to keep your machine from being overloaded.

**Vietnamese (SQL):**

```sql
-- Truy v·∫•n ƒë·ªÉ tr√≠ch xu·∫•t d·ªØ li·ªáu ChEMBL ƒë·ªÉ ph√¢n t√≠ch (gi·ªõi h·∫°n ·ªü 100 h√†ng)
-- L∆∞u truy v·∫•n n√†y th√†nh chembl_35_89_data.csv

SELECT
    cmp.chembl_id,
    md.molfile,
    act.standard_type,
    act.standard_value,
    act.standard_units,
    act.assay_chembl_id
FROM
    compound_structures cmp
JOIN
    molecule_dictionary md ON cmp.molregno = md.molregno
JOIN
    activities act ON md.molregno = act.molregno
WHERE
    act.standard_type IN ('IC50', 'Ki', 'EC50')  -- Xem x√©t c√°c lo·∫°i ho·∫°t ƒë·ªông li√™n quan
    AND act.standard_units = 'nM'  -- T·∫≠p trung v√†o ƒë∆°n v·ªã nM ƒë·ªÉ nh·∫•t qu√°n
    AND act.standard_value IS NOT NULL
    AND act.standard_value::text ~ '^[0-9.]+$'  -- ƒê·∫£m b·∫£o standard_value l√† s·ªë
LIMIT 100;  -- Gi·ªõi h·∫°n ·ªü 100 h√†ng
```

**Gi·∫£i th√≠ch (SQL):**

*   **S·ª≠a L·ªói:** L·ªói ban ƒë·∫ßu `ERROR: operator does not exist: numeric ~ unknown` x·∫£y ra v√¨ b·∫°n ƒëang c·ªë g·∫Øng s·ª≠ d·ª•ng to√°n t·ª≠ `~` (kh·ªõp bi·ªÉu th·ª©c ch√≠nh quy) tr√™n c·ªôt `numeric`.  PostgreSQL c·∫ßn m·ªôt ki·ªÉu chuy·ªÉn ƒë·ªïi r√µ r√†ng sang vƒÉn b·∫£n cho vi·ªác ƒë√≥.  D√≤ng `act.standard_value::text ~ '^[0-9.]+$'` hi·ªán chuy·ªÉn ƒë·ªïi ch√≠nh x√°c `standard_value` th√†nh vƒÉn b·∫£n tr∆∞·ªõc khi √°p d·ª•ng bi·ªÉu th·ª©c ch√≠nh quy.
*   **Ch·ªçn D·ªØ li·ªáu:** Ch·ªçn ID ChEMBL c·ªßa ph√¢n t·ª≠, molfile (chu·ªói SDF), lo·∫°i ho·∫°t ƒë·ªông, gi√° tr·ªã ti√™u chu·∫©n, ƒë∆°n v·ªã v√† ID x√©t nghi·ªám.
*   **L·ªçc:** L·ªçc theo c√°c lo·∫°i ho·∫°t ƒë·ªông ph·ªï bi·∫øn (IC50, Ki, EC50) v√† chu·∫©n h√≥a c√°c ƒë∆°n v·ªã th√†nh 'nM'. N√≥ c≈©ng ƒë·∫£m b·∫£o r·∫±ng `standard_value` kh√¥ng ph·∫£i l√† null v√† c√≥ th·ªÉ ƒë∆∞·ª£c bi·ªÉu di·ªÖn d∆∞·ªõi d·∫°ng m·ªôt s·ªë.
*   **Gi·ªõi h·∫°n:** Quan tr·ªçng l√† gi·ªõi h·∫°n k·∫øt qu·∫£ ·ªü 100 h√†ng ƒë·ªÉ tr√°nh m√°y c·ªßa b·∫°n b·ªã qu√° t·∫£i.

**b) Python (`notebook/Topic_CheMBL_35_89_1_data_loading.ipynb` - Data Loading & Processing)**

```python
import os
import pandas as pd
from rdkit import Chem
from rdkit.Chem import Descriptors
import numpy as np  # Import numpy

# Define base path (adjust as needed based on your project structure)
base_path = ".."  # Assuming the notebook is in the 'notebook' folder one level up from base
data_path = os.path.join(base_path, "data", "chembl_35_89_data.csv")

# Load the data
try:
    df = pd.read_csv(data_path)
except FileNotFoundError:
    print(f"Error: CSV file not found at {data_path}.  Make sure you ran the SQL script and saved the CSV.")
    exit()

print(f"Data loaded successfully.  Shape: {df.shape}")
print(df.head())

# Function to calculate molecular weight using RDKit
def calculate_molecular_weight(molfile):
    try:
        mol = Chem.MolFromMolBlock(molfile)
        if mol is not None:
            return Descriptors.MolWt(mol)
        else:
            return np.nan  # Handle cases where MolBlock parsing fails
    except Exception as e:
        print(f"Error processing MolBlock: {e}")
        return np.nan

# Apply the function to the dataframe
df['molecular_weight'] = df['molfile'].apply(calculate_molecular_weight)

# Display the dataframe with molecular weight
print(df.head())

# Handle missing values (NaN) if any
df = df.dropna()

print(f"Dataframe shape after removing NaN values: {df.shape}")


# Example of further processing: Convert standard_value to numeric and take -log10(IC50)
df = df[df['standard_type'] == 'IC50']  # Filter for IC50 values only for this example
df['standard_value'] = pd.to_numeric(df['standard_value'], errors='coerce')  # Convert to numeric, handling errors
df = df.dropna(subset=['standard_value'])  # Remove rows with NaN standard_value after conversion
df['pIC50'] = -np.log10(df['standard_value'] * 1e-9) # Convert nM to M and take -log10

print(df.head())
```

**Explanation (Python - Data Loading & Processing):**

*   **Import Libraries:** Imports necessary libraries: `os`, `pandas`, `RDKit` (`Chem`, `Descriptors`).
*   **Path Handling:** Uses `os.path.join` to construct the correct path to your CSV file.  This is important for reproducibility.
*   **Data Loading:**  Loads the CSV file into a Pandas DataFrame using `pd.read_csv`.  Includes error handling for the case where the file is not found.
*   **Molecular Weight Calculation:** Defines a function `calculate_molecular_weight` that takes a MolBlock string (from the `molfile` column), parses it using RDKit, and calculates the molecular weight.  It also includes error handling to deal with potentially invalid MolBlocks.
*   **Apply Function:** Applies the `calculate_molecular_weight` function to the `molfile` column to create a new column called `molecular_weight`.
*   **Handle Missing Values:** Removes rows with missing values (NaN) that might result from errors in MolBlock parsing.
*   **Example Processing (pIC50):** Demonstrates further data processing:
    *   Filters the DataFrame to include only rows where `standard_type` is 'IC50'.
    *   Converts the `standard_value` column to numeric using `pd.to_numeric`, handling potential errors by setting invalid values to `NaN`.
    *   Removes rows with `NaN` values in the `standard_value` column (resulting from the conversion).
    *   Calculates pIC50 values from IC50 (nM) values using the formula:  `pIC50 = -log10(IC50 * 1e-9)` (converting nM to M).

**Vietnamese (Python - T·∫£i v√† X·ª≠ l√Ω D·ªØ li·ªáu):**

```python
import os
import pandas as pd
from rdkit import Chem
from rdkit.Chem import Descriptors
import numpy as np  # Nh·∫≠p numpy

# X√°c ƒë·ªãnh ƒë∆∞·ªùng d·∫´n c∆° s·ªü (ƒëi·ªÅu ch·ªânh theo c·∫•u tr√∫c d·ª± √°n c·ªßa b·∫°n)
base_path = ".."  # Gi·∫£ s·ª≠ notebook n·∫±m trong th∆∞ m·ª•c 'notebook' m·ªôt c·∫•p tr√™n c∆° s·ªü
data_path = os.path.join(base_path, "data", "chembl_35_89_data.csv")

# T·∫£i d·ªØ li·ªáu
try:
    df = pd.read_csv(data_path)
except FileNotFoundError:
    print(f"L·ªói: Kh√¥ng t√¨m th·∫•y t·ªáp CSV t·∫°i {data_path}. ƒê·∫£m b·∫£o b·∫°n ƒë√£ ch·∫°y script SQL v√† l∆∞u CSV.")
    exit()

print(f"D·ªØ li·ªáu ƒë√£ ƒë∆∞·ª£c t·∫£i th√†nh c√¥ng. H√¨nh d·∫°ng: {df.shape}")
print(df.head())

# H√†m t√≠nh to√°n tr·ªçng l∆∞·ª£ng ph√¢n t·ª≠ b·∫±ng RDKit
def calculate_molecular_weight(molfile):
    try:
        mol = Chem.MolFromMolBlock(molfile)
        if mol is not None:
            return Descriptors.MolWt(mol)
        else:
            return np.nan  # X·ª≠ l√Ω c√°c tr∆∞·ªùng h·ª£p ph√¢n t√≠ch c√∫ ph√°p MolBlock kh√¥ng th√†nh c√¥ng
    except Exception as e:
        print(f"L·ªói khi x·ª≠ l√Ω MolBlock: {e}")
        return np.nan

# √Åp d·ª•ng h√†m cho dataframe
df['molecular_weight'] = df['molfile'].apply(calculate_molecular_weight)

# Hi·ªÉn th·ªã dataframe v·ªõi tr·ªçng l∆∞·ª£ng ph√¢n t·ª≠
print(df.head())

# X·ª≠ l√Ω c√°c gi√° tr·ªã b·ªã thi·∫øu (NaN) n·∫øu c√≥
df = df.dropna()

print(f"H√¨nh d·∫°ng c·ªßa Dataframe sau khi lo·∫°i b·ªè c√°c gi√° tr·ªã NaN: {df.shape}")

# V√≠ d·ª• v·ªÅ x·ª≠ l√Ω th√™m: Chuy·ªÉn ƒë·ªïi standard_value th√†nh s·ªë v√† l·∫•y -log10(IC50)
df = df[df['standard_type'] == 'IC50']  # L·ªçc ch·ªâ c√°c gi√° tr·ªã IC50 cho v√≠ d·ª• n√†y
df['standard_value'] = pd.to_numeric(df['standard_value'], errors='coerce')  # Chuy·ªÉn ƒë·ªïi th√†nh s·ªë, x·ª≠ l√Ω l·ªói
df = df.dropna(subset=['standard_value'])  # X√≥a c√°c h√†ng c√≥ standard_value NaN sau khi chuy·ªÉn ƒë·ªïi
df['pIC50'] = -np.log10(df['standard_value'] * 1e-9)  # Chuy·ªÉn ƒë·ªïi nM th√†nh M v√† l·∫•y -log10

print(df.head())
```

**Gi·∫£i th√≠ch (Python - T·∫£i v√† X·ª≠ l√Ω D·ªØ li·ªáu):**

*   **Nh·∫≠p Th∆∞ vi·ªán:** Nh·∫≠p c√°c th∆∞ vi·ªán c·∫ßn thi·∫øt: `os`, `pandas`, `RDKit` (`Chem`, `Descriptors`).
*   **X·ª≠ l√Ω ƒê∆∞·ªùng d·∫´n:** S·ª≠ d·ª•ng `os.path.join` ƒë·ªÉ x√¢y d·ª±ng ƒë∆∞·ªùng d·∫´n ch√≠nh x√°c ƒë·∫øn t·ªáp CSV c·ªßa b·∫°n. ƒêi·ªÅu n√†y r·∫•t quan tr·ªçng ƒë·ªÉ t√°i t·∫°o.
*   **T·∫£i D·ªØ li·ªáu:** T·∫£i t·ªáp CSV v√†o Pandas DataFrame b·∫±ng `pd.read_csv`. Bao g·ªìm x·ª≠ l√Ω l·ªói cho tr∆∞·ªùng h·ª£p kh√¥ng t√¨m th·∫•y t·ªáp.
*   **T√≠nh Tr·ªçng l∆∞·ª£ng Ph√¢n t·ª≠:** ƒê·ªãnh nghƒ©a m·ªôt h√†m `calculate_molecular_weight` l·∫•y m·ªôt chu·ªói MolBlock (t·ª´ c·ªôt `molfile`), ph√¢n t√≠ch c√∫ ph√°p n√≥ b·∫±ng RDKit v√† t√≠nh to√°n tr·ªçng l∆∞·ª£ng ph√¢n t·ª≠. N√≥ c≈©ng bao g·ªìm x·ª≠ l√Ω l·ªói ƒë·ªÉ ƒë·ªëi ph√≥ v·ªõi c√°c MolBlock c√≥ kh·∫£ nƒÉng kh√¥ng h·ª£p l·ªá.
*   **√Åp d·ª•ng H√†m:** √Åp d·ª•ng h√†m `calculate_molecular_weight` cho c·ªôt `molfile` ƒë·ªÉ t·∫°o m·ªôt c·ªôt m·ªõi c√≥ t√™n l√† `molecular_weight`.
*   **X·ª≠ l√Ω Gi√° tr·ªã Thi·∫øu:** Lo·∫°i b·ªè c√°c h√†ng c√≥ gi√° tr·ªã b·ªã thi·∫øu (NaN) c√≥ th·ªÉ ph√°t sinh t·ª´ l·ªói trong ph√¢n t√≠ch c√∫ ph√°p MolBlock.
*   **V√≠ d·ª• X·ª≠ l√Ω (pIC50):** Th·ªÉ hi·ªán x·ª≠ l√Ω d·ªØ li·ªáu th√™m:
    *   L·ªçc DataFrame ƒë·ªÉ ch·ªâ bao g·ªìm c√°c h√†ng c√≥ `standard_type` l√† 'IC50'.
    *   Chuy·ªÉn ƒë·ªïi c·ªôt `standard_value` th√†nh s·ªë b·∫±ng `pd.to_numeric`, x·ª≠ l√Ω c√°c l·ªói ti·ªÅm ·∫©n b·∫±ng c√°ch ƒë·∫∑t c√°c gi√° tr·ªã kh√¥ng h·ª£p l·ªá th√†nh `NaN`.
    *   X√≥a c√°c h√†ng c√≥ gi√° tr·ªã `NaN` trong c·ªôt `standard_value` (do chuy·ªÉn ƒë·ªïi).
    *   T√≠nh gi√° tr·ªã pIC50 t·ª´ c√°c gi√° tr·ªã IC50 (nM) b·∫±ng c√¥ng th·ª©c: `pIC50 = -log10(IC50 * 1e-9)` (chuy·ªÉn ƒë·ªïi nM th√†nh M).

**c) Python (`notebook/Topic_CheMBL_35_89_2_feature_generation.ipynb` - Feature Generation)**

```python
import os
import pandas as pd
from rdkit import Chem
from rdkit.Chem import AllChem
from rdkit.Chem import Descriptors

# Define base path
base_path = ".."
data_path = os.path.join(base_path, "data", "chembl_35_89_data.csv")

# Load the data
try:
    df = pd.read_csv(data_path)
except FileNotFoundError:
    print(f"Error: CSV file not found at {data_path}.")
    exit()

# Function to generate Morgan fingerprints (ECFP4)
def generate_morgan_fingerprint(molfile, radius=2, nBits=2048):
    try:
        mol = Chem.MolFromMolBlock(molfile)
        if mol is not None:
            fp = AllChem.GetMorganFingerprintAsBitVect(mol, radius, nBits=nBits)
            return fp
        else:
            return None
    except Exception as e:
        print(f"Error generating fingerprint: {e}")
        return None

# Generate Morgan fingerprints
df['morgan_fp'] = df['molfile'].apply(generate_morgan_fingerprint)

# Convert fingerprints to a list of integers (if fingerprint generation was successful)
df['morgan_fp_list'] = df['morgan_fp'].apply(lambda fp: list(fp) if fp is not None else None)

# Drop rows where fingerprint generation failed
df = df.dropna(subset=['morgan_fp_list'])

# Expand the fingerprint list into separate columns
fingerprint_df = pd.DataFrame(df['morgan_fp_list'].tolist(), index=df.index)
df = pd.concat([df, fingerprint_df], axis=1)

# Remove the original 'molfile' and intermediate fingerprint columns
df = df.drop(columns=['molfile', 'morgan_fp', 'morgan_fp_list'])

print(df.head())
print(f"Shape of the dataframe after feature generation: {df.shape}")
```

**Explanation (Python - Feature Generation):**

*   **Import Libraries:** Imports necessary libraries, including `AllChem` for fingerprint generation.
*   **Load Data:**  Loads the data from the CSV file (you might want to load the processed data from the previous notebook, depending on your workflow).
*   **`generate_morgan_fingerprint` function:**
    *   Takes a MolBlock string as input.
    *   Parses the MolBlock into an RDKit molecule object using `Chem.MolFromMolBlock()`.
    *   Generates a Morgan fingerprint (also known as ECFP4) using `AllChem.GetMorganFingerprintAsBitVect()`.  The `radius` and `nBits` parameters control the fingerprint's characteristics.
    *   Returns the fingerprint object.  Returns `None` if the molecule parsing fails.
*   **Apply Fingerprint Generation:** Applies the `generate_morgan_fingerprint` function to the `molfile` column to create a new column called `morgan_fp`.
*   **Convert to List:** Converts the fingerprint object to a list of integers (0 or 1) for each bit.  This makes it easier to use the fingerprint as features in machine learning models.
*   **Expand into Columns:**  The core part of turning the fingerprints into usable features:
    *   Creates a new DataFrame (`fingerprint_df`) from the list of fingerprints.  Each fingerprint bit becomes a separate column in this DataFrame.
    *   Concatenates the `fingerprint_df` with the original DataFrame `df`.
*   **Clean Up:** Removes the original `molfile` and intermediate fingerprint columns (`morgan_fp`, `morgan_fp_list`) since they are no longer needed.

**Vietnamese (Python - T·∫°o ƒê·∫∑c tr∆∞ng):**

```python
import os
import pandas as pd
from rdkit import Chem
from rdkit.Chem import AllChem
from rdkit.Chem import Descriptors

# X√°c ƒë·ªãnh ƒë∆∞·ªùng d·∫´n c∆° s·ªü
base_path = ".."
data_path = os.path.join(base_path, "data", "chembl_35_89_data.csv")

# T·∫£i d·ªØ li·ªáu
try:
    df = pd.read_csv(data_path)
except FileNotFoundError:
    print(f"L·ªói: Kh√¥ng t√¨m th·∫•y t·ªáp CSV t·∫°i {data_path}.")
    exit()

# H√†m t·∫°o v√¢n tay Morgan (ECFP4)
def generate_morgan_fingerprint(molfile, radius=2, nBits=2048):
    try:
        mol = Chem.MolFromMolBlock(molfile)
        if mol is not None:
            fp = AllChem.GetMorganFingerprintAsBitVect(mol, radius, nBits=nBits)
            return fp
        else:
            return None
    except Exception as e:
        print(f"L·ªói khi t·∫°o v√¢n tay: {e}")
        return None

# T·∫°o v√¢n tay Morgan
df['morgan_fp'] = df['molfile'].apply(generate_morgan_fingerprint)

# Chuy·ªÉn ƒë·ªïi v√¢n tay th√†nh danh s√°ch c√°c s·ªë nguy√™n (n·∫øu t·∫°o v√¢n tay th√†nh c√¥ng)
df['morgan_fp_list'] = df['morgan_fp'].apply(lambda fp: list(fp) if fp is not None else None)

# Lo·∫°i b·ªè c√°c h√†ng m√† vi·ªác t·∫°o v√¢n tay kh√¥ng th√†nh c√¥ng
df = df.dropna(subset=['morgan_fp_list'])

# M·ªü r·ªông danh s√°ch v√¢n tay th√†nh c√°c c·ªôt ri√™ng bi·ªát
fingerprint_df = pd.DataFrame(df['morgan_fp_list'].tolist(), index=df.index)
df = pd.concat([df, fingerprint_df], axis=1)

# Lo·∫°i b·ªè c·ªôt 'molfile' ban ƒë·∫ßu v√† c√°c c·ªôt v√¢n tay trung gian
df = df.drop(columns=['molfile', 'morgan_fp', 'morgan_fp_list'])

print(df.head())
print(f"H√¨nh d·∫°ng c·ªßa dataframe sau khi t·∫°o ƒë·∫∑c tr∆∞ng: {df.shape}")
```

**Gi·∫£i th√≠ch (Python - T·∫°o ƒê·∫∑c tr∆∞ng):**

*   **Nh·∫≠p Th∆∞ vi·ªán:** Nh·∫≠p c√°c th∆∞ vi·ªán c·∫ßn thi·∫øt, bao g·ªìm `AllChem` ƒë·ªÉ t·∫°o v√¢n tay.
*   **T·∫£i D·ªØ li·ªáu:** T·∫£i d·ªØ li·ªáu t·ª´ t·ªáp CSV (b·∫°n c√≥ th·ªÉ mu·ªën t·∫£i d·ªØ li·ªáu ƒë√£ x·ª≠ l√Ω t·ª´ notebook tr∆∞·ªõc ƒë√≥, t√πy thu·ªôc v√†o quy tr√¨nh l√†m vi·ªác c·ªßa b·∫°n).
*   **H√†m `generate_morgan_fingerprint`:**
    *   L·∫•y m·ªôt chu·ªói MolBlock l√†m ƒë·∫ßu v√†o.
    *   Ph√¢n t√≠ch c√∫ ph√°p MolBlock th√†nh m·ªôt ƒë·ªëi t∆∞·ª£ng ph√¢n t·ª≠ RDKit b·∫±ng c√°ch s·ª≠ d·ª•ng `Chem.MolFromMolBlock()`.
    *   T·∫°o v√¢n tay Morgan (c√≤n ƒë∆∞·ª£c g·ªçi l√† ECFP4) b·∫±ng c√°ch s·ª≠ d·ª•ng `AllChem.GetMorganFingerprintAsBitVect()`. C√°c tham s·ªë `radius` v√† `nBits` ki·ªÉm so√°t c√°c ƒë·∫∑c ƒëi·ªÉm c·ªßa v√¢n tay.
    *   Tr·∫£ v·ªÅ ƒë·ªëi t∆∞·ª£ng v√¢n tay. Tr·∫£ v·ªÅ `None` n·∫øu vi·ªác ph√¢n t√≠ch c√∫ ph√°p ph√¢n t·ª≠ kh√¥ng th√†nh c√¥ng.
*   **√Åp d·ª•ng T·∫°o V√¢n tay:** √Åp d·ª•ng h√†m `generate_morgan_fingerprint` cho c·ªôt `molfile` ƒë·ªÉ t·∫°o m·ªôt c·ªôt m·ªõi c√≥ t√™n l√† `morgan_fp`.
*   **Chuy·ªÉn ƒë·ªïi th√†nh Danh s√°ch:** Chuy·ªÉn ƒë·ªïi ƒë·ªëi t∆∞·ª£ng v√¢n tay th√†nh m·ªôt danh s√°ch c√°c s·ªë nguy√™n (0 ho·∫∑c 1) cho m·ªói bit. ƒêi·ªÅu n√†y gi√∫p b·∫°n d·ªÖ d√†ng s·ª≠ d·ª•ng v√¢n tay l√†m ƒë·∫∑c tr∆∞ng trong c√°c m√¥ h√¨nh h·ªçc m√°y.
*   **M·ªü r·ªông th√†nh C·ªôt:** Ph·∫ßn c·ªët l√µi c·ªßa vi·ªác bi·∫øn v√¢n tay th√†nh c√°c ƒë·∫∑c tr∆∞ng c√≥ th·ªÉ s·ª≠ d·ª•ng ƒë∆∞·ª£c:
    *   T·∫°o m·ªôt DataFrame m·ªõi (`fingerprint_df`) t·ª´ danh s√°ch v√¢n tay. M·ªói bit v√¢n tay tr·ªü th√†nh m·ªôt·ªôt ri√™ng bi·ªát trong DataFrame n√†y.
    *   N·ªëi `fingerprint_df` v·ªõi DataFrame ban ƒë·∫ßu `df`.
*   **D·ªçn d·∫πp:** Lo·∫°i b·ªè c·ªôt `molfile` ban ƒë·∫ßu v√† c√°c c·ªôt v√¢n tay trung gian (`morgan_fp`, `morgan_fp_list`) v√¨ ch√∫ng kh√¥ng c√≤n c·∫ßn thi·∫øt n·ªØa.

**d) Python (`notebook/Topic_CheMBL_35_89_3_model_building.ipynb` - Model Building)**

```python
import os
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error, r2_score
import joblib

# Define base path
base_path = ".."
data_path = os.path.join(base_path, "data", "chembl_35_89_data.csv") # Or load the processed data file from previous notebook

# Load the data
try:
    df = pd.read_csv(data_path) # Consider loading the processed data here if using pIC50 values
except FileNotFoundError:
    print(f"Error: CSV file not found at {data_path}.")
    exit()

# Drop rows with missing values
df = df.dropna()

# Prepare the data
X = df.iloc[:, 6:]  # Features:  Assuming fingerprint columns start from the 7th column onwards
y = df['standard_value'] # Target variable: IC50 values

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Initialize and train the Random Forest Regressor model
model = RandomForestRegressor(n_estimators=100, random_state=42)
model.fit(X_train, y_train)

# Make predictions on the test set
y_pred = model.predict(X_test)

# Evaluate the model
mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)

print(f'Mean Squared Error: {mse}')
print(f'R-squared: {r2}')

# Save the model
model_path = os.path.join(base_path, "models", "random_forest_model.joblib")
os.makedirs(os.path.dirname(model_path), exist_ok=True)
joblib.dump(model, model_path)

print(f"Model saved to: {model_path}")
```

**Explanation (Python - Model Building):**

*   **Import Libraries:** Imports necessary libraries for model building, evaluation, and saving.
*   **Load Data:** Loads the data from the CSV file (or the processed data from the previous step).  **Important:** Make sure this data includes the features you generated (e.g., the fingerprint columns).
*   **Prepare Data:**
    *   Defines `X` (the features) and `y` (the target variable).  **Crucially, adjust the column indexing for `X` to match the columns where your features (fingerprints) are located.** The example assumes they start from the 7th column onwards.
    *   `y` is set to `'standard_value'` in this example. If you created and want to use `pIC50`, change this line to `y = df['pIC50']`.
*   **Split Data:** Splits the data into training and testing sets using `train_test_split`.
*   **Train Model:**
    *   Initializes a `RandomForestRegressor` model. You can experiment with different models and hyperparameters.
    *   Trains the model using the training data (`model.fit`).
*   **Evaluate Model:**
    *   Makes predictions on the test set (`model.predict`).
    *   Calculates the Mean Squared Error (MSE) and R-squared (R2) to evaluate the model's performance.
*   **Save Model:** Saves the trained model to a file using `joblib.dump`.  This allows you to load the model later without retraining it.  The model is saved in a `models` directory.

**Vietnamese (Python - X√¢y d·ª±ng M√¥ h√¨nh):**

```python
import os
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error, r2_score
import joblib

# X√°c ƒë·ªãnh ƒë∆∞·ªùng d·∫´n c∆° s·ªü
base_path = ".."
data_path = os.path.join(base_path, "data", "chembl_35_89_data.csv")  # Ho·∫∑c t·∫£i t·ªáp d·ªØ li·ªáu ƒë√£ x·ª≠ l√Ω t·ª´ notebook tr∆∞·ªõc ƒë√≥

# T·∫£i d·ªØ li·ªáu
try:
    df = pd.read_csv(data_path)  # C√¢n nh·∫Øc t·∫£i d·ªØ li·ªáu ƒë√£ x·ª≠ l√Ω ·ªü ƒë√¢y n·∫øu s·ª≠ d·ª•ng gi√° tr·ªã pIC50
except FileNotFoundError:
    print(f"L·ªói: Kh√¥ng t√¨m th·∫•y t·ªáp CSV t·∫°i {data_path}.")
    exit()

# Lo·∫°i b·ªè c√°c h√†ng c√≥ gi√° tr·ªã b·ªã thi·∫øu
df = df.dropna()

# Chu·∫©n b·ªã d·ªØ li·ªáu
X = df.iloc[:, 6:]  # ƒê·∫∑c tr∆∞ng: Gi·∫£ s·ª≠ c√°c c·ªôt v√¢n tay b·∫Øt ƒë·∫ßu t·ª´ c·ªôt th·ª© 7 tr·ªü ƒëi
y = df['standard_value']  # Bi·∫øn m·ª•c ti√™u: Gi√° tr·ªã IC50

# Chia d·ªØ li·ªáu th√†nh c√°c t·∫≠p hu·∫•n luy·ªán v√† ki·ªÉm tra
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Kh·ªüi t·∫°o v√† hu·∫•n luy·ªán m√¥ h√¨nh Random Forest Regressor
model = RandomForestRegressor(n_estimators=100, random_state=42)
model.fit(X_train, y_train)

# ƒê∆∞a ra d·ª± ƒëo√°n tr√™n t·∫≠p ki·ªÉm tra
y_pred = model.predict(X_test)

# ƒê√°nh gi√° m√¥ h√¨nh
mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)

print(f'Mean Squared Error: {mse}')
print(f'R-squared: {r2}')

# L∆∞u m√¥ h√¨nh
model_path = os.path.join(base_path, "models", "random_forest_model.joblib")
os.makedirs(os.path.dirname(model_path), exist_ok=True)
joblib.dump(model, model_path)

print(f"M√¥ h√¨nh ƒë√£ ƒë∆∞·ª£c l∆∞u v√†o: {model_path}")
```

**Gi·∫£i th√≠ch (Python - X√¢y d·ª±ng M√¥ h√¨nh):**

*   **Nh·∫≠p Th∆∞ vi·ªán:** Nh·∫≠p c√°c th∆∞ vi·ªán c·∫ßn thi·∫øt cho vi·ªác x√¢y d·ª±ng, ƒë√°nh gi√° v√† l∆∞u m√¥ h√¨nh.
*   **T·∫£i D·ªØ li·ªáu:** T·∫£i d·ªØ li·ªáu t·ª´ t·ªáp CSV (ho·∫∑c d·ªØ li·ªáu ƒë√£ x·ª≠ l√Ω t·ª´ b∆∞·ªõc tr∆∞·ªõc). **Quan tr·ªçng:** ƒê·∫£m b·∫£o d·ªØ li·ªáu n√†y bao g·ªìm c√°c ƒë·∫∑c tr∆∞ng b·∫°n ƒë√£ t·∫°o (v√≠ d·ª•: c√°c c·ªôt v√¢n tay).
*   **Chu·∫©n b·ªã D·ªØ li·ªáu:**
    *   X√°c ƒë·ªãnh `X` (c√°c ƒë·∫∑c tr∆∞ng) v√† `y` (bi·∫øn m·ª•c ti√™u). **Quan tr·ªçng, ƒëi·ªÅu ch·ªânh ch·ªâ m·ª•c c·ªôt cho `X` ƒë·ªÉ kh·ªõp v·ªõi c√°c c·ªôt n∆°i c√°c ƒë·∫∑c tr∆∞ng (v√¢n tay) c·ªßa b·∫°n ƒë∆∞·ª£c ƒë·∫∑t.** V√≠ d·ª• gi·∫£ ƒë·ªãnh ch√∫ng b·∫Øt ƒë·∫ßu t·ª´ c·ªôt th·ª© 7 tr·ªü ƒëi.
    *   `y` ƒë∆∞·ª£c ƒë·∫∑t th√†nh `'standard_value'` trong v√≠ d·ª• n√†y. N·∫øu b·∫°n ƒë√£ t·∫°o v√† mu·ªën s·ª≠ d·ª•ng `pIC50`, h√£y thay ƒë·ªïi d√≤ng n√†y th√†nh `y = df['pIC50']`.
*   **Chia D·ªØ li·ªáu:** Chia d·ªØ li·ªáu th√†nh c√°c t·∫≠p hu·∫•n luy·ªán v√† ki·ªÉm tra b·∫±ng c√°ch s·ª≠ d·ª•ng `train_test_split`.
*   **Hu·∫•n luy·ªán M√¥ h√¨nh:**
    *   Kh·ªüi t·∫°o m·ªôt m√¥ h√¨nh `RandomForestRegressor`. B·∫°n c√≥ th·ªÉ th·ª≠ nghi·ªám v·ªõi c√°c m√¥ h√¨nh v√† si√™u tham s·ªë kh√°c nhau.
    *   Hu·∫•n luy·ªán m√¥ h√¨nh b·∫±ng c√°ch s·ª≠ d·ª•ng d·ªØ li·ªáu hu·∫•n luy·ªán (`model.fit`).
*   **ƒê√°nh gi√° M√¥ h√¨nh:**
    *   ƒê∆∞a ra d·ª± ƒëo√°n tr√™n t·∫≠p ki·ªÉm tra (`model.predict`).
    *   T√≠nh to√°n Mean Squared Error (MSE) v√† R-squared (R2) ƒë·ªÉ ƒë√°nh gi√° hi·ªáu su·∫•t c·ªßa m√¥ h√¨nh.
*   **L∆∞u M√¥ h√¨nh:** L∆∞u m√¥ h√¨nh ƒë√£ ƒë∆∞·ª£c hu·∫•n luy·ªán v√†o m·ªôt t·ªáp b·∫±ng c√°ch s·ª≠ d·ª•ng `joblib.dump`. ƒêi·ªÅu n√†y cho ph√©p b·∫°n t·∫£i m√¥ h√¨nh sau n√†y m√† kh√¥ng c·∫ßn hu·∫•n luy·ªán l·∫°i. M√¥ h√¨nh ƒë∆∞·ª£c l∆∞u trong th∆∞ m·ª•c `models`.

**3. Addressing the `squared=False` error:**

The error `old scikit-learn version does not support parameters squared=False in the mean_squared_error function` indicates that you are using an older version of scikit-learn.  The `squared=False` parameter was introduced in a later version.

**Solution:**

1.  **Update scikit-learn:** The best solution is to update your scikit-learn version.  In your Jupyter notebook, run:

    ```bash
    !pip install --upgrade scikit-learn
    ```

    Restart your Jupyter kernel after upgrading.

2.  **Alternative (if upgrading is not possible):**  If you absolutely cannot upgrade, you can manually calculate the Root Mean Squared Error (RMSE) instead of relying on the `squared=False` parameter:

    ```python
    from sklearn.metrics import mean_squared_error
    import numpy as np

    mse = mean_squared_error(y_test, y_pred)
    rmse = np.sqrt(mse)  # Calculate RMSE manually
    print(f"Root Mean Squared Error: {rmse}")
    ```

**4. Project Folder Structure**

```
Topic_CheMBL_35_89/
‚îú‚îÄ‚îÄ data/
‚îÇ   ‚îî‚îÄ‚îÄ chembl_35_89_data.csv  (Output from SQL query)
‚îú‚îÄ‚îÄ notebooks/
‚îÇ   ‚îú‚îÄ‚îÄ Topic_CheMBL_35_89_1_data_loading.ipynb
‚îÇ   ‚îú‚îÄ‚îÄ Topic_CheMBL_35_89_2_feature_generation.ipynb
‚îÇ   ‚îî‚îÄ‚îÄ Topic_CheMBL_35_89_3_model_building.ipynb
‚îú‚îÄ‚îÄ models/
‚îÇ   ‚îî‚îÄ‚îÄ random_forest_model.joblib
‚îî‚îÄ‚îÄ README.md (Optional: Project description and instructions)
```

**5. Five Examples of Using the Code (Example Use Cases)**

**Example 1: Basic Molecular Weight Calculation and Saving to a New CSV**

*   **Goal:** Calculate molecular weight for the 100 compounds and save the data with the calculated molecular weights to a new CSV file.

```python
# In notebook/Topic_CheMBL_35_89_1_data_loading.ipynb

import os
import pandas as pd
from rdkit import Chem
from rdkit.Chem import Descriptors
import numpy as np

# ... (Data loading and molecular weight calculation code from above) ...

# Save the updated DataFrame to a new CSV file
output_path = os.path.join(base_path, "data", "chembl_35_89_data_with_mw.csv")
df.to_csv(output_path, index=False)  # index=False prevents writing the DataFrame index to the CSV
print(f"Data with molecular weights saved to: {output_path}")
```

**Vietnamese:**

*   **M·ª•c ti√™u:** T√≠nh tr·ªçng l∆∞·ª£ng ph√¢n t·ª≠ cho 100 h·ª£p ch·∫•t v√† l∆∞u d·ªØ li·ªáu c√πng v·ªõi tr·ªçng l∆∞·ª£ng ph√¢n t·ª≠ ƒë√£ t√≠nh v√†o m·ªôt t·ªáp CSV m·ªõi.

```python
# Trong notebook/Topic_CheMBL_35_89_1_data_loading.ipynb

import os
import pandas as pd
from rdkit import Chem
from rdkit.Chem import Descriptors
import numpy as np

# ... (M√£ t·∫£i d·ªØ li·ªáu v√† t√≠nh tr·ªçng l∆∞·ª£ng ph√¢n t·ª≠ t·ª´ tr√™n) ...

# L∆∞u DataFrame ƒë√£ c·∫≠p nh·∫≠t v√†o m·ªôt t·ªáp CSV m·ªõi
output_path = os.path.join(base_path, "data", "chembl_35_89_data_with_mw.csv")
df.to_csv(output_path, index=False)  # index=False ngƒÉn kh√¥ng cho ghi ch·ªâ m·ª•c DataFrame v√†o CSV
print(f"D·ªØ li·ªáu c√≥ tr·ªçng l∆∞·ª£ng ph√¢n t·ª≠ ƒë√£ ƒë∆∞·ª£c l∆∞u v√†o: {output_path}")
```

**Example 2: Generating and Visualizing a Single Molecular Fingerprint**

*   **Goal:** Generate a Morgan fingerprint for the first molecule in your dataset and visualize its bit representation.

```python
# In notebook/Topic_CheMBL_35_89_2_feature_generation.ipynb

import os
import pandas as pd
from rdkit import Chem
from rdkit.Chem import AllChem
from rdkit.Chem.Draw import DrawMorganBit

# ... (Data loading and fingerprint generation code from above) ...

# Get the first molecule and its Morgan fingerprint
first_molfile = df['molfile'].iloc[0]
mol = Chem.MolFromMolBlock(first_molfile)
fp = AllChem.GetMorganFingerprint(mol, 2) #Different for visualization

# Find the most significant bit(s)
info = {}
bits = fp.GetNonzeroElements()
print(bits)

#Draw Significant bits
for bit in bits:
    img = DrawMorganBit(mol,bit,info)
    img.save(f"bit_{bit}.png") #save each bit

```

**Vietnamese:**

*   **M·ª•c ti√™u:** T·∫°o v√¢n tay Morgan cho ph√¢n t·ª≠ ƒë·∫ßu ti√™n trong t·∫≠p d·ªØ li·ªáu c·ªßa b·∫°n v√† tr·ª±c quan h√≥a bi·ªÉu di·ªÖn bit c·ªßa n√≥.

```python
# Trong notebook/Topic_CheMBL_35_89_2_feature_generation.ipynb

import os
import pandas as pd
from rdkit import Chem
from rdkit.Chem import AllChem
from rdkit.Chem.Draw import DrawMorganBit

# ... (M√£ t·∫£i d·ªØ li·ªáu v√† t·∫°o v√¢n tay t·ª´ tr√™n) ...

# L·∫•y ph√¢n t·ª≠ ƒë·∫ßu ti√™n v√† v√¢n tay Morgan c·ªßa n√≥
first_molfile = df['molfile'].iloc[0]
mol = Chem.MolFromMolBlock(first_molfile)
fp = AllChem.GetMorganFingerprint(mol, 2) #Kh√°c nhau ƒë·ªÉ tr·ª±c quan h√≥a

# T√¨m (c√°c) bit quan tr·ªçng nh·∫•t
info = {}
bits = fp.GetNonzeroElements()
print(bits)

#V·∫Ω c√°c bit quan tr·ªçng
for bit in bits:
    img = DrawMorganBit(mol,bit,info)
    img.save(f"bit_{bit}.png") #l∆∞u t·ª´ng bit
```

**Example 3: Filtering by Activity Value**

*   **Goal:** Filter the DataFrame to only include compounds with a standard value less than 100 nM.

```python
# In notebook/Topic_CheMBL_35_89_1_data_loading.ipynb (or a new notebook)

import os
import pandas as pd

# ... (Data loading code from above) ...

# Filter for compounds with standard_value < 100 nM
df_filtered = df[df['standard_value'] < 100]
print(f"Shape of filtered DataFrame: {df_filtered.shape}")
print(df_filtered.head())
```

**Vietnamese:**

*   **M·ª•c ti√™u:** L·ªçc DataFrame ƒë·ªÉ ch·ªâ bao g·ªìm c√°c h·ª£p ch·∫•t c√≥ gi√° tr·ªã ti√™u chu·∫©n nh·ªè h∆°n 100 nM.

```python
# Trong notebook/Topic_CheMBL_35_89_1_data_loading.ipynb (ho·∫∑c m·ªôt notebook m·ªõi)

import os
import pandas as pd

# ... (M√£ t·∫£i d·ªØ li·ªáu t·ª´ tr√™n) ...

# L·ªçc c√°c h·ª£p ch·∫•t c√≥ standard_value < 100 nM
df_filtered = df[df['standard_value'] < 100]
print(f"H√¨nh d·∫°ng c·ªßa DataFrame ƒë√£ l·ªçc: {df_filtered.shape}")
print(df_filtered.head())
```

**Example 4: Train a model with pIC50 values and evaluate with RMSE**

*   **Goal:** Calculate the pIC50 values, train and evaluate the mode, report the RMSE

```python
# In notebook/Topic_CheMBL_35_89_3_model_building.ipynb

import os
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error, r2_score
import joblib
import numpy as np

# Define base path
base_path = ".."
data_path = os.path.join(base_path, "data", "chembl_35_89_data.csv") # Or load the processed data file from previous notebook

# Load the data
try:
    df = pd.read_csv(data_path) # Consider loading the processed data here if using pIC50 values
except FileNotFoundError:
    print(f"Error: CSV file not found at {data_path}.")
    exit()

# Drop rows with missing values
df = df.dropna()

# Prepare the data
X = df.iloc[:, 6:]  # Features:  Assuming fingerprint columns start from the 7th column onwards
y = df['pIC50'] # Target variable: pIC50 values

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Initialize and train the Random Forest Regressor model
model = RandomForestRegressor(n_estimators=100, random_state=42)
model.fit(X_train, y_train)

# Make predictions on the test set
y_pred = model.predict(X_test)

# Evaluate the model
mse = mean_squared_error(y_test, y_pred)
rmse = np.sqrt(mse) #Calculate RMSE
r2 = r2_score(y_test, y_pred)

print(f'Root Mean Squared Error: {rmse}')
print(f'R-squared: {r2}')

# Save the model
model_path = os.path.join(base_path, "models", "random_forest_model.joblib")
os.makedirs(os.path.dirname(model_path), exist_ok=True)
joblib.dump(model, model_path)

print(f"Model saved to: {model_path}")
```

**Vietnamese:**

*   **M·ª•c ti√™u:** T√≠nh gi√° tr·ªã pIC50, hu·∫•n luy·ªán v√† ƒë√°nh gi√° m√¥ h√¨nh, b√°o c√°o RMSE

```python
# Trong notebook/Topic_CheMBL_35_89_3_model_building.ipynb

import os
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error, r2_score
import joblib
import numpy as np

# X√°c ƒë·ªãnh ƒë∆∞·ªùng d·∫´n c∆° s·ªü
base_path = ".."
data_path = os.path.join(base_path, "data", "chembl_35_89_data.csv")  # Ho·∫∑c t·∫£i t·ªáp d·ªØ li·ªáu ƒë√£ x·ª≠ l√Ω t·ª´ notebook tr∆∞·ªõc ƒë√≥

# T·∫£i d·ªØ li·ªáu
try:
    df = pd.read_csv(data_path)  # C√¢n nh·∫Øc t·∫£i d·ªØ li·ªáu ƒë√£ x·ª≠ l√Ω ·ªü ƒë√¢y n·∫øu s·ª≠ d·ª•ng gi√° tr·ªã pIC50
except FileNotFoundError:
    print(f"L·ªói: Kh√¥ng t√¨m th·∫•y t·ªáp CSV t·∫°i {data_path}.")
    exit()

# Lo·∫°i b·ªè c√°c h√†ng c√≥ gi√° tr·ªã b·ªã thi·∫øu
df = df.dropna()

# Chu·∫©n b·ªã d·ªØ li·ªáu
X = df.iloc[:, 6:]  # ƒê·∫∑c tr∆∞ng: Gi·∫£ s·ª≠ c√°c c·ªôt v√¢n tay b·∫Øt ƒë·∫ßu t·ª´ c·ªôt th·ª© 7 tr·ªü ƒëi
y = df['pIC50']  # Bi·∫øn m·ª•c ti√™u: Gi√° tr·ªã pIC50

# Chia d·ªØ li·ªáu th√†nh c√°c t·∫≠p hu·∫•n luy·ªán v√† ki·ªÉm tra
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Kh·ªüi t·∫°o v√† hu·∫•n luy·ªán m√¥ h√¨nh Random Forest Regressor
model = RandomForestRegressor(n_estimators=100, random_state=42)
model.fit(X_train, y_train)

# ƒê∆∞a ra d·ª± ƒëo√°n tr√™n t·∫≠p ki·ªÉm tra
y_pred = model.predict(X_test)

# ƒê√°nh gi√° m√¥ h√¨nh
mse = mean_squared_error(y_test, y_pred)
rmse = np.sqrt(mse)  # T√≠nh RMSE
r2 = r2_score(y_test, y_pred)

print(f'Root Mean Squared Error: {rmse}')
print(f'R-squared: {r2}')

# L∆∞u m√¥ h√¨nh
model_path = os.path.join(base_path, "models", "random_forest_model.joblib")
os.makedirs(os.path.dirname(model_path), exist_ok=True)
joblib.dump(model, model_path)

print(f"M√¥ h√¨nh ƒë√£ ƒë∆∞·ª£c l∆∞u v√†o: {model_path}")
```

**Example 5: Loading a Saved Model and Making Predictions on New Data**

*   **Goal:** Load the saved Random Forest model and use it to predict activity values for a small set of new molecules (assuming you have their MolBlocks).

```python
# In a new notebook (or the model building notebook)

import os
import pandas as pd
import joblib
from rdkit import Chem
from rdkit.Chem import AllChem
import numpy as np

# Define base path
base_path = ".."
model_path = os.path.join(base_path, "models", "random_forest_model.joblib")

# Load the model
try:
    model = joblib.load(model_path)
    print("Model loaded successfully.")
except FileNotFoundError:
    print(f"Error: Model file not found at {model_path}")
    exit()

# Sample new data (replace with your actual data)
new_data = {
    'molfile': [
        """
        MolBlock for molecule 1...
        """,
        """
        MolBlock for molecule 2...
        """,
    ]
}
new_df = pd.DataFrame(new_data)

def generate_morgan_fingerprint(molfile, radius=2, nBits=2048):
    try:
        mol = Chem.MolFromMolBlock(molfile)
        if mol is not None:
            fp = AllChem.GetMorganFingerprintAsBitVect(mol, radius, nBits=nBits)
            return fp
        else:
            return None
    except Exception as e:
        print(f"Error generating fingerprint: {e}")
        return None

# Generate Morgan fingerprints
new_df['morgan_fp'] = new_df['molfile'].apply(generate_morgan_fingerprint)

# Convert fingerprints to a list of integers (if fingerprint generation was successful)
new_df['morgan_fp_list'] = new_df['morgan_fp'].apply(lambda fp: list(fp) if fp is not None else None)

# Drop rows where fingerprint generation failed
new_df = new_df.dropna(subset=['morgan_fp_list'])

# Expand the fingerprint list into separate columns
fingerprint_df = pd.DataFrame(new_df['morgan_fp_list'].tolist(), index=new_df.index)
new_df = pd.concat([new_df, fingerprint_df], axis=1)

# Prepare the data
X_new = new_df.iloc[:, 2:] # Features:  Adjust indexing based on where features start

# Make predictions
predictions = model.predict(X_new)

print("Predictions:")
for i, pred in enumerate(predictions):
    print(f"Molecule {i+1}: Predicted activity = {pred}")
```

**Vietnamese:**

*   **M·ª•c ti√™u:** T·∫£i m√¥ h√¨nh Random Forest ƒë√£ l∆∞u v√† s·ª≠ d·ª•ng n√≥ ƒë·ªÉ d·ª± ƒëo√°n gi√° tr·ªã ho·∫°t ƒë·ªông cho m·ªôt t·∫≠p h·ª£p nh·ªè c√°c ph√¢n t·ª≠ m·ªõi (gi·∫£ s·ª≠ b·∫°n c√≥ MolBlock c·ªßa ch√∫ng).

```python
# Trong m·ªôt notebook m·ªõi (ho·∫∑c notebook x√¢y d·ª±ng m√¥ h√¨nh)

import os
import pandas as pd
import joblib
from rdkit import Chem
from rdkit.Chem import AllChem
import numpy as np

# X√°c ƒë·ªãnh ƒë∆∞·ªùng d·∫´n c∆° s·ªü
base_path = ".."
model_path = os.path.join(base_path, "models", "random_forest_model.joblib")

# T·∫£i m√¥ h√¨nh
try:
    model = joblib.load(model_path)
    print("M√¥ h√¨nh ƒë√£ ƒë∆∞·ª£c t·∫£i th√†nh c√¥ng.")
except FileNotFoundError:
    print(f"L·ªói: Kh√¥ng t√¨m th·∫•y t·ªáp m√¥ h√¨nh t·∫°i {model_path}")
    exit()

# D·ªØ li·ªáu m·ªõi m·∫´u (thay th·∫ø b·∫±ng d·ªØ li·ªáu th·ª±c t·∫ø c·ªßa b·∫°n)
new_data = {
    'molfile': [
        """
        MolBlock cho ph√¢n t·ª≠ 1...
        """,
        """
        MolBlock cho ph√¢n t·ª≠ 2...
        """,
    ]
}
new_df = pd.DataFrame(new_data)

def generate_morgan_fingerprint(molfile, radius=2, nBits=2048):
    try:
        mol = Chem.MolFromMolBlock(molfile)
        if mol is not None:
            fp = AllChem.GetMorganFingerprintAsBitVect(mol, radius, nBits=nBits)
            return fp
        else:
            return None
    except Exception as e:
        print(f"L·ªói khi t·∫°o v√¢n tay: {e}")
        return None

# T·∫°o v√¢n tay Morgan
new_df['morgan_fp'] = new_df['molfile'].apply(generate_morgan_fingerprint)

# Chuy·ªÉn ƒë·ªïi v√¢n tay th√†nh danh s√°ch c√°c s·ªë nguy√™n (n·∫øu t·∫°o v√¢n tay th√†nh c√¥ng)
new_df['morgan_fp_list'] = new_df['morgan_fp'].apply(lambda fp: list(fp) if fp is not None else None)

# Lo·∫°i b·ªè c√°c h√†ng m√† vi·ªác t·∫°o v√¢n tay kh√¥ng th√†nh c√¥ng
new_df = new_df.dropna(subset=['morgan_fp_list'])

# M·ªü r·ªông danh s√°ch v√¢n tay th√†nh c√°c c·ªôt ri√™ng bi·ªát
fingerprint_df = pd.DataFrame(new_df['morgan_fp_list'].tolist(), index=new_df.index)
new_df = pd.concat([new_df, fingerprint_df], axis=1)


# Chu·∫©n b·ªã d·ªØ li·ªáu
X_new = new_df.iloc[:, 2:] # ƒê·∫∑c tr∆∞ng:  ƒêi·ªÅu ch·ªânh ch·ªâ m·ª•c d·ª±a tr√™n v·ªã tr√≠ b·∫Øt ƒë·∫ßu c·ªßa ƒë·∫∑c tr∆∞ng

# ƒê∆∞a ra d·ª± ƒëo√°n
predictions = model.predict(X_new)

print("D·ª± ƒëo√°n:")
for i, pred in enumerate(predictions):
    print(f"Ph√¢n t·ª≠ {i+1}: Ho·∫°t ƒë·ªông d·ª± ƒëo√°n = {pred}")
```

**Key Considerations & Best Practices:**

*   **Error Handling:** I've included basic error handling (e.g., `try...except` blocks for file loading and MolBlock parsing).  You should expand this to handle other potential issues.
*   **Data Validation:**  Thoroughly validate your data after loading it.  Check for inconsistencies, outliers, and unexpected values.
*   **Feature Selection:**  Consider using feature selection techniques to identify the most relevant features for your model. This can improve performance and reduce overfitting.
*   **Hyperparameter Tuning:**  Experiment with different hyperparameters for your machine learning models to optimize their performance.  Techniques like grid search or randomized search can be helpful.
*   **Cross-Validation:**  Use cross-validation to get a more robust estimate of your model's performance.
*   **Logging:**  Implement logging to track the progress of your analysis and to record any errors or warnings.
*   **Reproducibility:**  Use a consistent project structure, version control (e.g., Git), and a virtual environment to ensure that your analysis is reproducible.
*   **Workflow Management:** Consider using a workflow management system (e.g., Airflow, Prefect) to automate your data analysis pipeline.

Remember to adapt the code to your specific needs and experiment with different models, features, and parameters. Good luck with your project!
