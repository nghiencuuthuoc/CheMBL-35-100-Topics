# PharmApp Suite
## üß† AI for Drug Discovery and Development üß™
| Copyright 2025 | Nghi√™n C·ª©u Thu·ªëc | www.nghiencuuthuoc.com | Zalo: +84888999311 |

üß© Topic: Khai th√°c ch·∫•t ·ª©c ch·∫ø kinase t·ª´ ChEMBL
---
Tuy·ªát v·ªùi! T√¥i r·∫•t vui ƒë∆∞·ª£c h·ªó tr·ª£ b·∫°n trong d·ª± √°n ph√¢n t√≠ch d·ªØ li·ªáu ChEMBL 35 k·∫øt h·ª£p RDKit. V·ªõi vai tr√≤ l√† m·ªôt chuy√™n gia nghi√™n c·ª©u v√† ph√°t tri·ªÉn d∆∞·ª£c h·ªçc, t√¥i s·∫Ω cung c·∫•p cho b·∫°n nh·ªØng h∆∞·ªõng d·∫´n chi ti·∫øt, code m·∫´u SQL v√† Python, c≈©ng nh∆∞ c√°c v√≠ d·ª• c·ª• th·ªÉ ƒë·ªÉ b·∫°n c√≥ th·ªÉ tri·ªÉn khai d·ª± √°n m·ªôt c√°ch hi·ªáu qu·∫£.

**1. Ph√¢n t√≠ch m√¥ h√¨nh ph√¢n t√≠ch (Analysis of the Analysis Model)**

D·ª±a tr√™n th√¥ng tin b·∫°n cung c·∫•p, t√¥i hi·ªÉu r·∫±ng b·∫°n ƒëang mu·ªën:

*   **Tr√≠ch xu·∫•t d·ªØ li·ªáu t·ª´ c∆° s·ªü d·ªØ li·ªáu ChEMBL 35:** S·ª≠ d·ª•ng SQL ƒë·ªÉ truy v·∫•n v√† l·ªçc d·ªØ li·ªáu li√™n quan ƒë·∫øn ho·∫°t t√≠nh sinh h·ªçc c·ªßa c√°c h·ª£p ch·∫•t.
*   **Ti·ªÅn x·ª≠ l√Ω d·ªØ li·ªáu:** S·ª≠ d·ª•ng Python v√† RDKit ƒë·ªÉ x·ª≠ l√Ω d·ªØ li·ªáu, bao g·ªìm chuy·ªÉn ƒë·ªïi SMILES sang fingerprint, t√≠nh to√°n c√°c thu·ªôc t√≠nh v·∫≠t l√Ω h√≥a h·ªçc, v√† x·ª≠ l√Ω c√°c gi√° tr·ªã b·ªã thi·∫øu.
*   **Ph√¢n t√≠ch d·ªØ li·ªáu:** S·ª≠ d·ª•ng c√°c m√¥ h√¨nh th·ªëng k√™ v√† h·ªçc m√°y ƒë·ªÉ kh√°m ph√° c√°c m·ªëi quan h·ªá gi·ªØa c·∫•u tr√∫c h√≥a h·ªçc v√† ho·∫°t t√≠nh sinh h·ªçc.
*   **L∆∞u tr·ªØ v√† qu·∫£n l√Ω d·ªØ li·ªáu:** S·ª≠ d·ª•ng c·∫•u tr√∫c th∆∞ m·ª•c AIMLOps ƒë·ªÉ qu·∫£n l√Ω code, d·ªØ li·ªáu v√† k·∫øt qu·∫£.

**M√¥ h√¨nh ph√¢n t√≠ch ƒë·ªÅ xu·∫•t:**

T√¥i ƒë·ªÅ xu·∫•t m·ªôt quy tr√¨nh ph√¢n t√≠ch g·ªìm c√°c b∆∞·ªõc sau:

1.  **Data Extraction and Cleaning:**
    *   **SQL:** Extract relevant data from ChEMBL 35 (e.g., compounds, activities, targets).
    *   **Python:** Handle missing values, convert data types, and filter out irrelevant entries.
2.  **Feature Engineering:**
    *   **RDKit:** Generate molecular fingerprints (e.g., Morgan fingerprints) and calculate physicochemical properties (e.g., LogP, MW, TPSA).
3.  **Data Analysis:**
    *   **Exploratory Data Analysis (EDA):** Visualize data distributions, identify outliers, and explore relationships between variables.
    *   **Model Building:** Train machine learning models to predict activity based on molecular features. Potential models include:
        *   **Regression Models:** Linear Regression, Random Forest Regression, Support Vector Regression.
        *   **Classification Models:** Logistic Regression, Random Forest Classification, Support Vector Classification.
4.  **Model Evaluation:**
    *   Evaluate model performance using appropriate metrics (e.g., R-squared, RMSE for regression; accuracy, precision, recall, F1-score for classification).
    *   Use cross-validation to ensure model generalizability.
5.  **Interpretation and Reporting:**
    *   Interpret model results to identify important features and understand the structure-activity relationship.
    *   Generate reports and visualizations to communicate findings.

**2. H∆∞·ªõng d·∫´n song ng·ªØ (Bilingual Instructions)**

**English:**

This project aims to analyze ChEMBL 35 data to understand the relationship between chemical structure and biological activity. We will use SQL to extract data, RDKit to generate molecular features, and machine learning models to predict activity. The AIMLOps structure will be used to organize code, data, and results.

**Ti·∫øng Vi·ªát:**

D·ª± √°n n√†y nh·∫±m m·ª•c ƒë√≠ch ph√¢n t√≠ch d·ªØ li·ªáu ChEMBL 35 ƒë·ªÉ hi·ªÉu m·ªëi quan h·ªá gi·ªØa c·∫•u tr√∫c h√≥a h·ªçc v√† ho·∫°t t√≠nh sinh h·ªçc. Ch√∫ng ta s·∫Ω s·ª≠ d·ª•ng SQL ƒë·ªÉ tr√≠ch xu·∫•t d·ªØ li·ªáu, RDKit ƒë·ªÉ t·∫°o ra c√°c ƒë·∫∑c tr∆∞ng ph√¢n t·ª≠ v√† c√°c m√¥ h√¨nh h·ªçc m√°y ƒë·ªÉ d·ª± ƒëo√°n ho·∫°t t√≠nh. C·∫•u tr√∫c AIMLOps s·∫Ω ƒë∆∞·ª£c s·ª≠ d·ª•ng ƒë·ªÉ t·ªï ch·ª©c code, d·ªØ li·ªáu v√† k·∫øt qu·∫£.

**3. Code SQL v√† Python (SQL and Python Code)**

**SQL (English):**

```sql
-- Extract 100 compounds with activity data for a specific target
SELECT DISTINCT
    md.molregno,
    cs.canonical_smiles,
    act.standard_value,
    act.standard_units,
    act.standard_type
FROM
    molecule_dictionary md
JOIN
    compound_structures cs ON md.molregno = cs.molregno
JOIN
    activities act ON md.molregno = act.molregno
JOIN
    target_dictionary td ON act.tid = td.tid
WHERE
    td.pref_name = 'Acetylcholinesterase'  -- Replace with your target of interest
    AND act.standard_type = 'IC50'
    AND act.standard_units = 'nM'
    AND act.standard_value IS NOT NULL
    AND act.standard_value > 0
    AND act.standard_value::text ~ '^[0-9\.]+$' -- Ensure standard_value is numeric
LIMIT 100;
```

**SQL (Ti·∫øng Vi·ªát):**

```sql
-- Tr√≠ch xu·∫•t 100 h·ª£p ch·∫•t c√≥ d·ªØ li·ªáu ho·∫°t t√≠nh cho m·ªôt m·ª•c ti√™u c·ª• th·ªÉ
SELECT DISTINCT
    md.molregno,
    cs.canonical_smiles,
    act.standard_value,
    act.standard_units,
    act.standard_type
FROM
    molecule_dictionary md
JOIN
    compound_structures cs ON md.molregno = cs.molregno
JOIN
    activities act ON md.molregno = act.molregno
JOIN
    target_dictionary td ON act.tid = td.tid
WHERE
    td.pref_name = 'Acetylcholinesterase'  -- Thay th·∫ø b·∫±ng m·ª•c ti√™u b·∫°n quan t√¢m
    AND act.standard_type = 'IC50'
    AND act.standard_units = 'nM'
    AND act.standard_value IS NOT NULL
    AND act.standard_value > 0
    AND act.standard_value::text ~ '^[0-9\.]+$' -- ƒê·∫£m b·∫£o standard_value l√† s·ªë
LIMIT 100;
```

**L∆∞u √Ω:** ƒêo·∫°n code SQL tr√™n ƒë√£ ƒë∆∞·ª£c s·ª≠a ƒë·ªÉ kh·∫Øc ph·ª•c l·ªói `ERROR: operator does not exist: numeric ~ unknown`. Ch√∫ng ta ƒë√£ chuy·ªÉn ƒë·ªïi `act.standard_value` sang ki·ªÉu text v√† s·ª≠ d·ª•ng to√°n t·ª≠ `~` ƒë·ªÉ so s√°nh v·ªõi m·ªôt bi·ªÉu th·ª©c ch√≠nh quy.

**Python (English):**

```python
import os
import pandas as pd
from rdkit import Chem
from rdkit.Chem import AllChem
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.preprocessing import StandardScaler

# Define the base path for your project
base_path = "path/to/your/project" # Replace with your actual path

# Load the data from the CSV file
data_path = os.path.join(base_path, "data", "chembl_data.csv") # Replace "chembl_data.csv" with your actual filename
df = pd.read_csv(data_path)

# Handle missing values (replace with the mean)
df['standard_value'] = pd.to_numeric(df['standard_value'], errors='coerce')
df['standard_value'] = df['standard_value'].fillna(df['standard_value'].mean())

# Convert IC50 to pIC50
df['pIC50'] = -np.log10(df['standard_value'] / 1e9)

# Function to calculate Morgan fingerprints using RDKit
def calculate_morgan_fingerprint(smiles, radius=2, nBits=2048):
    mol = Chem.MolFromSmiles(smiles)
    if mol:
        fp = AllChem.GetMorganFingerprintAsBitVect(mol, radius, nBits=nBits)
        return np.array(list(fp.ToBitString()), dtype=int)
    else:
        return None

# Apply the function to create a new column with fingerprints
df['fingerprint'] = df['canonical_smiles'].apply(calculate_morgan_fingerprint)

# Drop rows where fingerprint calculation failed
df = df.dropna(subset=['fingerprint'])

# Convert fingerprints to a matrix
X = np.vstack(df['fingerprint'].to_numpy())
y = df['pIC50'].to_numpy()

# Data scaling
scaler = StandardScaler()
X = scaler.fit_transform(X)

# Split data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Train a Random Forest Regressor model
model = RandomForestRegressor(n_estimators=100, random_state=42)
model.fit(X_train, y_train)

# Make predictions on the test set
y_pred = model.predict(X_test)

# Evaluate the model
mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)

print(f"Mean Squared Error: {mse}")
print(f"R-squared: {r2}")
```

**Python (Ti·∫øng Vi·ªát):**

```python
import os
import pandas as pd
from rdkit import Chem
from rdkit.Chem import AllChem
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.preprocessing import StandardScaler

# X√°c ƒë·ªãnh ƒë∆∞·ªùng d·∫´n g·ªëc c·ªßa d·ª± √°n
base_path = "path/to/your/project" # Thay th·∫ø b·∫±ng ƒë∆∞·ªùng d·∫´n th·ª±c t·∫ø c·ªßa b·∫°n

# T·∫£i d·ªØ li·ªáu t·ª´ file CSV
data_path = os.path.join(base_path, "data", "chembl_data.csv") # Thay th·∫ø "chembl_data.csv" b·∫±ng t√™n file th·ª±c t·∫ø c·ªßa b·∫°n
df = pd.read_csv(data_path)

# X·ª≠ l√Ω gi√° tr·ªã b·ªã thi·∫øu (thay th·∫ø b·∫±ng gi√° tr·ªã trung b√¨nh)
df['standard_value'] = pd.to_numeric(df['standard_value'], errors='coerce')
df['standard_value'] = df['standard_value'].fillna(df['standard_value'].mean())

# Chuy·ªÉn ƒë·ªïi IC50 sang pIC50
df['pIC50'] = -np.log10(df['standard_value'] / 1e9)

# H√†m t√≠nh to√°n Morgan fingerprint s·ª≠ d·ª•ng RDKit
def calculate_morgan_fingerprint(smiles, radius=2, nBits=2048):
    mol = Chem.MolFromSmiles(smiles)
    if mol:
        fp = AllChem.GetMorganFingerprintAsBitVect(mol, radius, nBits=nBits)
        return np.array(list(fp.ToBitString()), dtype=int)
    else:
        return None

# √Åp d·ª•ng h√†m ƒë·ªÉ t·∫°o m·ªôt c·ªôt m·ªõi v·ªõi fingerprint
df['fingerprint'] = df['canonical_smiles'].apply(calculate_morgan_fingerprint)

# Lo·∫°i b·ªè c√°c h√†ng m√† t√≠nh to√°n fingerprint th·∫•t b·∫°i
df = df.dropna(subset=['fingerprint'])

# Chuy·ªÉn ƒë·ªïi fingerprint th√†nh ma tr·∫≠n
X = np.vstack(df['fingerprint'].to_numpy())
y = df['pIC50'].to_numpy()

# Chu·∫©n h√≥a d·ªØ li·ªáu
scaler = StandardScaler()
X = scaler.fit_transform(X)

# Chia d·ªØ li·ªáu th√†nh t·∫≠p hu·∫•n luy·ªán v√† t·∫≠p ki·ªÉm tra
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Hu·∫•n luy·ªán m√¥ h√¨nh Random Forest Regressor
model = RandomForestRegressor(n_estimators=100, random_state=42)
model.fit(X_train, y_train)

# D·ª± ƒëo√°n tr√™n t·∫≠p ki·ªÉm tra
y_pred = model.predict(X_test)

# ƒê√°nh gi√° m√¥ h√¨nh
mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)

print(f"Mean Squared Error: {mse}")
print(f"R-squared: {r2}")
```

**L∆∞u √Ω:** ƒêo·∫°n code Python tr√™n ƒë√£ ƒë∆∞·ª£c c·∫≠p nh·∫≠t ƒë·ªÉ:

*   S·ª≠ d·ª•ng `StandardScaler` ƒë·ªÉ chu·∫©n h√≥a d·ªØ li·ªáu tr∆∞·ªõc khi hu·∫•n luy·ªán m√¥ h√¨nh.
*   S·ª≠ d·ª•ng `mean_squared_error` v√† `r2_score` t·ª´ `sklearn.metrics` ƒë·ªÉ ƒë√°nh gi√° m√¥ h√¨nh.
*   Lo·∫°i b·ªè tham s·ªë `squared=False` trong h√†m `mean_squared_error` (n·∫øu b·∫°n ƒëang s·ª≠ d·ª•ng phi√™n b·∫£n scikit-learn c≈©, h√£y n√¢ng c·∫•p l√™n phi√™n b·∫£n m·ªõi nh·∫•t).
*   X·ª≠ l√Ω gi√° tr·ªã b·ªã thi·∫øu b·∫±ng c√°ch thay th·∫ø ch√∫ng b·∫±ng gi√° tr·ªã trung b√¨nh c·ªßa c·ªôt `standard_value`.
*   Chuy·ªÉn ƒë·ªïi gi√° tr·ªã IC50 sang pIC50 b·∫±ng c√¥ng th·ª©c `pIC50 = -np.log10(IC50 / 1e9)`.

**4. V√≠ d·ª• code SQL v√† Python (SQL and Python Code Examples)**

**V√≠ d·ª• 1: Tr√≠ch xu·∫•t d·ªØ li·ªáu cho m·ªôt h·ªç protein c·ª• th·ªÉ (Extract data for a specific protein family)**

**SQL (English):**

```sql
-- Extract compounds active against a specific protein family (e.g., Kinases)
SELECT DISTINCT
    md.molregno,
    cs.canonical_smiles,
    act.standard_value,
    act.standard_units,
    act.standard_type
FROM
    molecule_dictionary md
JOIN
    compound_structures cs ON md.molregno = cs.molregno
JOIN
    activities act ON md.molregno = act.molregno
JOIN
    target_dictionary td ON act.tid = td.tid
JOIN
    component_sequences cs2 ON td.component_id = cs2.component_id
WHERE
    cs2.protein_family_desc LIKE '%Kinase%'
    AND act.standard_type = 'IC50'
    AND act.standard_units = 'nM'
    AND act.standard_value IS NOT NULL
    AND act.standard_value > 0
    AND act.standard_value::text ~ '^[0-9\.]+$'
LIMIT 100;
```

**SQL (Ti·∫øng Vi·ªát):**

```sql
-- Tr√≠ch xu·∫•t c√°c h·ª£p ch·∫•t c√≥ ho·∫°t t√≠nh ch·ªëng l·∫°i m·ªôt h·ªç protein c·ª• th·ªÉ (v√≠ d·ª•: Kinase)
SELECT DISTINCT
    md.molregno,
    cs.canonical_smiles,
    act.standard_value,
    act.standard_units,
    act.standard_type
FROM
    molecule_dictionary md
JOIN
    compound_structures cs ON md.molregno = cs.molregno
JOIN
    activities act ON md.molregno = act.molregno
JOIN
    target_dictionary td ON act.tid = td.tid
JOIN
    component_sequences cs2 ON td.component_id = cs2.component_id
WHERE
    cs2.protein_family_desc LIKE '%Kinase%'
    AND act.standard_type = 'IC50'
    AND act.standard_units = 'nM'
    AND act.standard_value IS NOT NULL
    AND act.standard_value > 0
    AND act.standard_value::text ~ '^[0-9\.]+$'
LIMIT 100;
```

**V√≠ d·ª• 2: T√≠nh to√°n c√°c thu·ªôc t√≠nh v·∫≠t l√Ω h√≥a h·ªçc (Calculate physicochemical properties)**

**Python (English):**

```python
from rdkit import Chem
from rdkit.Chem import Descriptors

def calculate_physicochemical_properties(smiles):
    mol = Chem.MolFromSmiles(smiles)
    if mol:
        mw = Descriptors.MolWt(mol)
        logp = Descriptors.MolLogP(mol)
        hbd = Descriptors.NumHDonors(mol)
        hba = Descriptors.NumHAcceptors(mol)
        tpsa = Descriptors.TPSA(mol)
        return mw, logp, hbd, hba, tpsa
    else:
        return None, None, None, None, None

df[['MW', 'LogP', 'HBD', 'HBA', 'TPSA']] = df['canonical_smiles'].apply(lambda x: pd.Series(calculate_physicochemical_properties(x)))
```

**Python (Ti·∫øng Vi·ªát):**

```python
from rdkit import Chem
from rdkit.Chem import Descriptors

def calculate_physicochemical_properties(smiles):
    mol = Chem.MolFromSmiles(smiles)
    if mol:
        mw = Descriptors.MolWt(mol)
        logp = Descriptors.MolLogP(mol)
        hbd = Descriptors.NumHDonors(mol)
        hba = Descriptors.NumHAcceptors(mol)
        tpsa = Descriptors.TPSA(mol)
        return mw, logp, hbd, hba, tpsa
    else:
        return None, None, None, None, None

df[['MW', 'LogP', 'HBD', 'HBA', 'TPSA']] = df['canonical_smiles'].apply(lambda x: pd.Series(calculate_physicochemical_properties(x)))
```

**V√≠ d·ª• 3: X√¢y d·ª±ng m√¥ h√¨nh h·ªìi quy tuy·∫øn t√≠nh (Build a linear regression model)**

**Python (English):**

```python
from sklearn.linear_model import LinearRegression

# Train a Linear Regression model
model = LinearRegression()
model.fit(X_train, y_train)

# Make predictions on the test set
y_pred = model.predict(X_test)

# Evaluate the model
mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)

print(f"Mean Squared Error: {mse}")
print(f"R-squared: {r2}")
```

**Python (Ti·∫øng Vi·ªát):**

```python
from sklearn.linear_model import LinearRegression

# Hu·∫•n luy·ªán m√¥ h√¨nh Linear Regression
model = LinearRegression()
model.fit(X_train, y_train)

# D·ª± ƒëo√°n tr√™n t·∫≠p ki·ªÉm tra
y_pred = model.predict(X_test)

# ƒê√°nh gi√° m√¥ h√¨nh
mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)

print(f"Mean Squared Error: {mse}")
print(f"R-squared: {r2}")
```

**V√≠ d·ª• 4: S·ª≠ d·ª•ng PCA ƒë·ªÉ gi·∫£m chi·ªÅu d·ªØ li·ªáu (Use PCA for dimensionality reduction)**

**Python (English):**

```python
from sklearn.decomposition import PCA

# Apply PCA to reduce the number of features
pca = PCA(n_components=100)  # Reduce to 100 components
X_pca = pca.fit_transform(X)

# Split data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X_pca, y, test_size=0.2, random_state=42)

# Train a Random Forest Regressor model
model = RandomForestRegressor(n_estimators=100, random_state=42)
model.fit(X_train, y_train)

# Make predictions on the test set
y_pred = model.predict(X_test)

# Evaluate the model
mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)

print(f"Mean Squared Error: {mse}")
print(f"R-squared: {r2}")
```

**Python (Ti·∫øng Vi·ªát):**

```python
from sklearn.decomposition import PCA

# √Åp d·ª•ng PCA ƒë·ªÉ gi·∫£m s·ªë l∆∞·ª£ng ƒë·∫∑c tr∆∞ng
pca = PCA(n_components=100)  # Gi·∫£m xu·ªëng c√≤n 100 th√†nh ph·∫ßn
X_pca = pca.fit_transform(X)

# Chia d·ªØ li·ªáu th√†nh t·∫≠p hu·∫•n luy·ªán v√† t·∫≠p ki·ªÉm tra
X_train, X_test, y_train, y_test = train_test_split(X_pca, y, test_size=0.2, random_state=42)

# Hu·∫•n luy·ªán m√¥ h√¨nh Random Forest Regressor
model = RandomForestRegressor(n_estimators=100, random_state=42)
model.fit(X_train, y_train)

# D·ª± ƒëo√°n tr√™n t·∫≠p ki·ªÉm tra
y_pred = model.predict(X_test)

# ƒê√°nh gi√° m√¥ h√¨nh
mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)

print(f"Mean Squared Error: {mse}")
print(f"R-squared: {r2}")
```

**V√≠ d·ª• 5: S·ª≠ d·ª•ng Grid Search ƒë·ªÉ t·ªëi ∆∞u h√≥a si√™u tham s·ªë (Use Grid Search for hyperparameter optimization)**

**Python (English):**

```python
from sklearn.model_selection import GridSearchCV

# Define the parameter grid
param_grid = {
    'n_estimators': [50, 100, 200],
    'max_depth': [5, 10, 15],
    'min_samples_split': [2, 5, 10]
}

# Create a GridSearchCV object
grid_search = GridSearchCV(estimator=RandomForestRegressor(random_state=42),
                           param_grid=param_grid,
                           cv=3,
                           scoring='neg_mean_squared_error',
                           verbose=2)

# Fit the grid search to the data
grid_search.fit(X_train, y_train)

# Print the best parameters and the best score
print(f"Best parameters: {grid_search.best_params_}")
print(f"Best score: {-grid_search.best_score_}")

# Use the best model to make predictions
best_model = grid_search.best_estimator_
y_pred = best_model.predict(X_test)

# Evaluate the model
mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)

print(f"Mean Squared Error: {mse}")
print(f"R-squared: {r2}")
```

**Python (Ti·∫øng Vi·ªát):**

```python
from sklearn.model_selection import GridSearchCV

# X√°c ƒë·ªãnh l∆∞·ªõi tham s·ªë
param_grid = {
    'n_estimators': [50, 100, 200],
    'max_depth': [5, 10, 15],
    'min_samples_split': [2, 5, 10]
}

# T·∫°o ƒë·ªëi t∆∞·ª£ng GridSearchCV
grid_search = GridSearchCV(estimator=RandomForestRegressor(random_state=42),
                           param_grid=param_grid,
                           cv=3,
                           scoring='neg_mean_squared_error',
                           verbose=2)

# Fit grid search v√†o d·ªØ li·ªáu
grid_search.fit(X_train, y_train)

# In ra c√°c tham s·ªë t·ªët nh·∫•t v√† ƒëi·ªÉm s·ªë t·ªët nh·∫•t
print(f"Tham s·ªë t·ªët nh·∫•t: {grid_search.best_params_}")
print(f"ƒêi·ªÉm s·ªë t·ªët nh·∫•t: {-grid_search.best_score_}")

# S·ª≠ d·ª•ng m√¥ h√¨nh t·ªët nh·∫•t ƒë·ªÉ ƒë∆∞a ra d·ª± ƒëo√°n
best_model = grid_search.best_estimator_
y_pred = best_model.predict(X_test)

# ƒê√°nh gi√° m√¥ h√¨nh
mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)

print(f"Mean Squared Error: {mse}")
print(f"R-squared: {r2}")
```

Hy v·ªçng nh·ªØng h∆∞·ªõng d·∫´n v√† v√≠ d·ª• n√†y s·∫Ω gi√∫p b·∫°n tri·ªÉn khai d·ª± √°n ph√¢n t√≠ch d·ªØ li·ªáu ChEMBL 35 m·ªôt c√°ch th√†nh c√¥ng. N·∫øu b·∫°n c√≥ b·∫•t k·ª≥ c√¢u h·ªèi n√†o kh√°c, ƒë·ª´ng ng·∫ßn ng·∫°i h·ªèi!