# PharmApp Suite
## üß† AI for Drug Discovery and Development üß™
| Copyright 2025 | Nghi√™n C·ª©u Thu·ªëc | www.nghiencuuthuoc.com | Zalo: +84888999311 |

üß© Topic: ƒê·ªì th·ªã m·∫°ng l∆∞·ªõi target-ligand b·∫±ng NetworkX
---
Tuy·ªát v·ªùi! T√¥i r·∫•t vui ƒë∆∞·ª£c h·ªó tr·ª£ b·∫°n trong d·ª± √°n ph√¢n t√≠ch d·ªØ li·ªáu ChEMBL 35 s·ª≠ d·ª•ng RDKit. V·ªõi vai tr√≤ chuy√™n gia R&D d∆∞·ª£c h·ªçc, t√¥i s·∫Ω gi√∫p b·∫°n x√¢y d·ª±ng quy tr√¨nh ph√¢n t√≠ch hi·ªáu qu·∫£, t·ª´ truy v·∫•n SQL ƒë·∫øn x·ª≠ l√Ω v√† m√¥ h√¨nh h√≥a d·ªØ li·ªáu b·∫±ng Python.

D∆∞·ªõi ƒë√¢y l√† ph√¢n t√≠ch chi ti·∫øt v√† h∆∞·ªõng d·∫´n song ng·ªØ cho ch·ªß ƒë·ªÅ "Topic_CheMBL_35_76", c√πng v·ªõi c√°c v√≠ d·ª• code SQL v√† Python m·∫´u, gi√∫p b·∫°n b·∫Øt ƒë·∫ßu d·ª± √°n m·ªôt c√°ch su√¥n s·∫ª:

**1. Ph√¢n t√≠ch v√† M√¥ h√¨nh Ph√¢n t√≠ch (Analysis and Analytical Model)**

*   **M·ª•c ti√™u (Objective):**

    *   Ph√¢n t√≠ch m·ªëi quan h·ªá gi·ªØa c·∫•u tr√∫c h√≥a h·ªçc (chemical structure) v√† ho·∫°t t√≠nh sinh h·ªçc (biological activity) c·ªßa c√°c h·ª£p ch·∫•t trong c∆° s·ªü d·ªØ li·ªáu ChEMBL 35.
    *   X√¢y d·ª±ng m√¥ h√¨nh d·ª± ƒëo√°n ho·∫°t t√≠nh d·ª±a tr√™n c·∫•u tr√∫c ph√¢n t·ª≠.
*   **D·ªØ li·ªáu (Data):**

    *   D·ªØ li·ªáu t·ª´ c∆° s·ªü d·ªØ li·ªáu ChEMBL 35, bao g·ªìm th√¥ng tin v·ªÅ h·ª£p ch·∫•t (compounds), ho·∫°t t√≠nh sinh h·ªçc (bioactivities), m·ª•c ti√™u (targets), v√† c√°c thu·ªôc t√≠nh li√™n quan.
    *   S·ª≠ d·ª•ng RDKit ƒë·ªÉ t√≠nh to√°n c√°c descriptor ph√¢n t·ª≠ (molecular descriptors) t·ª´ c·∫•u tr√∫c h√≥a h·ªçc (SMILES).
*   **Ph∆∞∆°ng ph√°p (Methodology):**

    1.  **Truy v·∫•n v√† Tr√≠ch xu·∫•t D·ªØ li·ªáu (Data Query and Extraction):**
        *   S·ª≠ d·ª•ng SQL ƒë·ªÉ truy v·∫•n d·ªØ li·ªáu li√™n quan t·ª´ c∆° s·ªü d·ªØ li·ªáu ChEMBL 35.
        *   L·ªçc d·ªØ li·ªáu d·ª±a tr√™n c√°c ti√™u ch√≠ c·ª• th·ªÉ (v√≠ d·ª•: lo·∫°i ho·∫°t t√≠nh, m·ª•c ti√™u, gi√° tr·ªã ho·∫°t t√≠nh).
    2.  **Ti·ªÅn x·ª≠ l√Ω D·ªØ li·ªáu (Data Preprocessing):**
        *   L√†m s·∫°ch v√† chu·∫©n h√≥a d·ªØ li·ªáu.
        *   X·ª≠ l√Ω c√°c gi√° tr·ªã thi·∫øu (missing values) v√† ngo·∫°i l·ªá (outliers).
    3.  **T√≠nh to√°n Descriptor Ph√¢n t·ª≠ (Molecular Descriptor Calculation):**
        *   S·ª≠ d·ª•ng RDKit ƒë·ªÉ chuy·ªÉn ƒë·ªïi c·∫•u tr√∫c SMILES th√†nh c√°c descriptor ph√¢n t·ª≠ (v√≠ d·ª•: tr·ªçng l∆∞·ª£ng ph√¢n t·ª≠, logP, s·ªë l∆∞·ª£ng li√™n k·∫øt, v.v.).
    4.  **Ph√¢n t√≠ch Th·ªëng k√™ (Statistical Analysis):**
        *   Th·ª±c hi·ªán ph√¢n t√≠ch th·ªëng k√™ m√¥ t·∫£ (descriptive statistics) ƒë·ªÉ hi·ªÉu r√µ h∆°n v·ªÅ d·ªØ li·ªáu.
        *   S·ª≠ d·ª•ng c√°c ph∆∞∆°ng ph√°p th·ªëng k√™ (v√≠ d·ª•: h·ªìi quy tuy·∫øn t√≠nh, t∆∞∆°ng quan) ƒë·ªÉ x√°c ƒë·ªãnh m·ªëi quan h·ªá gi·ªØa descriptor v√† ho·∫°t t√≠nh.
    5.  **M√¥ h√¨nh h√≥a (Modeling):**
        *   X√¢y d·ª±ng m√¥ h√¨nh h·ªçc m√°y (machine learning models) ƒë·ªÉ d·ª± ƒëo√°n ho·∫°t t√≠nh sinh h·ªçc d·ª±a tr√™n c√°c descriptor ph√¢n t·ª≠.
        *   S·ª≠ d·ª•ng c√°c thu·∫≠t to√°n nh∆∞ Random Forest, Support Vector Machines (SVM), ho·∫∑c Neural Networks.
    6.  **ƒê√°nh gi√° M√¥ h√¨nh (Model Evaluation):**
        *   ƒê√°nh gi√° hi·ªáu su·∫•t c·ªßa m√¥ h√¨nh b·∫±ng c√°ch s·ª≠ d·ª•ng c√°c ch·ªâ s·ªë ph√π h·ª£p (v√≠ d·ª•: R-squared, RMSE, AUC).
        *   S·ª≠ d·ª•ng cross-validation ƒë·ªÉ ƒë·∫£m b·∫£o t√≠nh t·ªïng qu√°t c·ªßa m√¥ h√¨nh.

**2. H∆∞·ªõng d·∫´n Song ng·ªØ (Bilingual Instructions)**

*   **SQL:**

    *   *Ti·∫øng Anh:* Use SQL to query and extract relevant data from the ChEMBL 35 database. Filter data based on specific criteria (e.g., activity type, target, activity values).
    *   *Ti·∫øng Vi·ªát:* S·ª≠ d·ª•ng SQL ƒë·ªÉ truy v·∫•n v√† tr√≠ch xu·∫•t d·ªØ li·ªáu li√™n quan t·ª´ c∆° s·ªü d·ªØ li·ªáu ChEMBL 35. L·ªçc d·ªØ li·ªáu d·ª±a tr√™n c√°c ti√™u ch√≠ c·ª• th·ªÉ (v√≠ d·ª•: lo·∫°i ho·∫°t t√≠nh, m·ª•c ti√™u, gi√° tr·ªã ho·∫°t t√≠nh).
*   **Python:**

    *   *Ti·∫øng Anh:* Use RDKit to calculate molecular descriptors from SMILES structures. Preprocess data to handle missing values and outliers. Build and evaluate machine learning models to predict biological activity.
    *   *Ti·∫øng Vi·ªát:* S·ª≠ d·ª•ng RDKit ƒë·ªÉ t√≠nh to√°n c√°c descriptor ph√¢n t·ª≠ t·ª´ c·∫•u tr√∫c SMILES. Ti·ªÅn x·ª≠ l√Ω d·ªØ li·ªáu ƒë·ªÉ x·ª≠ l√Ω c√°c gi√° tr·ªã thi·∫øu v√† ngo·∫°i l·ªá. X√¢y d·ª±ng v√† ƒë√°nh gi√° c√°c m√¥ h√¨nh h·ªçc m√°y ƒë·ªÉ d·ª± ƒëo√°n ho·∫°t t√≠nh sinh h·ªçc.

**3. V√≠ d·ª• Code (Code Examples)**

**SQL (English & Vietnamese)**

```sql
-- English: SQL query to extract 100 compounds with IC50 values for a specific target (e.g., CHEMBL244)
-- Vietnamese: Truy v·∫•n SQL ƒë·ªÉ tr√≠ch xu·∫•t 100 h·ª£p ch·∫•t c√≥ gi√° tr·ªã IC50 cho m·ªôt m·ª•c ti√™u c·ª• th·ªÉ (v√≠ d·ª•: CHEMBL244)
SELECT DISTINCT ON (cmpd.chembl_id)
    cmpd.chembl_id,
    act.standard_value,
    act.standard_units,
    act.standard_type,
    mol.smiles
FROM
    compound_structures AS mol
    JOIN activities AS act ON mol.molregno = act.molregno
    JOIN assays AS ass ON act.assay_id = ass.assay_id
    JOIN target_dictionary AS td ON ass.tid = td.tid
    JOIN molecule_dictionary AS cmpd ON mol.molregno = cmpd.molregno
WHERE
    td.chembl_id = 'CHEMBL244' -- Replace with your target of interest / Thay th·∫ø b·∫±ng m·ª•c ti√™u b·∫°n quan t√¢m
    AND act.standard_type = 'IC50'
    AND act.standard_units = 'nM'
    AND act.standard_value IS NOT NULL
    AND act.standard_value > 0
    AND mol.smiles IS NOT NULL
    AND act.standard_value ~ '^[0-9\.]+$' -- Ensure standard_value is numeric / ƒê·∫£m b·∫£o standard_value l√† s·ªë
LIMIT 100;
```

**Python (English & Vietnamese)**

```python
# English: Python code to read the extracted data, calculate molecular descriptors using RDKit, and train a Random Forest model
# Vietnamese: M√£ Python ƒë·ªÉ ƒë·ªçc d·ªØ li·ªáu ƒë√£ tr√≠ch xu·∫•t, t√≠nh to√°n descriptor ph√¢n t·ª≠ b·∫±ng RDKit v√† hu·∫•n luy·ªán m√¥ h√¨nh Random Forest

import pandas as pd
from rdkit import Chem
from rdkit.Chem import Descriptors
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error, r2_score
import numpy as np
import os

# Define base path
base_path = "." # Assuming the notebook is in the root directory
data_path = os.path.join(base_path, "data")

# Load data from CSV
try:
    df = pd.read_csv(os.path.join(data_path, "chembl_ic50_data.csv")) # Adjust filename accordingly
except FileNotFoundError:
    print(f"Error: File not found at {os.path.join(data_path, 'chembl_ic50_data.csv')}")
    exit()

# Function to calculate molecular descriptors
def calculate_descriptors(smiles):
    mol = Chem.MolFromSmiles(smiles)
    if mol is None:
        return None
    descriptors = {}
    for name, func in Descriptors.descList:
        try:
            descriptors[name] = func(mol)
        except:
            descriptors[name] = None  # Handle potential errors during descriptor calculation
    return descriptors

# Apply descriptor calculation
df['descriptors'] = df['smiles'].apply(calculate_descriptors)

# Handle errors and exclude rows with None descriptors
df = df.dropna(subset=['descriptors'])

# Convert descriptors to a DataFrame
descriptors_df = pd.DataFrame(df['descriptors'].tolist())

# Check for completely empty columns
empty_cols = descriptors_df.columns[descriptors_df.isnull().all()]
descriptors_df.drop(empty_cols, axis=1, inplace=True) #Remove empty columns

# Combine descriptors with activity data
X = descriptors_df
y = df['standard_value'].astype(float)  # Convert to numeric, handle potential conversion errors

# Split data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Handle missing values using imputation (replace NaN with mean)
from sklearn.impute import SimpleImputer
imputer = SimpleImputer(strategy='mean')
X_train = imputer.fit_transform(X_train)
X_test = imputer.transform(X_test)

# Train a Random Forest model
model = RandomForestRegressor(n_estimators=100, random_state=42)
model.fit(X_train, y_train)

# Make predictions
y_pred = model.predict(X_test)

# Evaluate the model
mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)

print(f"Mean Squared Error: {mse}")
print(f"R-squared: {r2}")
```

**Gi·∫£i th√≠ch l·ªói v√† c√°ch kh·∫Øc ph·ª•c (Error Explanation and Fixes)**

*   **L·ªói SQL (SQL Error):**

    *   *English:* `ERROR: operator does not exist: numeric ~ unknown, LINE 12: AND act.standard_value ~ '^[0-9\.]+$'`
    *   *Vietnamese:* `L·ªói: kh√¥ng t·ªìn t·∫°i to√°n t·ª≠: numeric ~ unknown, D√íNG 12: V√Ä act.standard_value ~ '^[0-9\.]+$'`
    *   *Explanation:* The `~` operator is used for regular expression matching, but PostgreSQL might not be able to implicitly convert the `standard_value` column to a text type for the regex comparison.
    *   *Fix:* Cast the `standard_value` column to `TEXT` before applying the regex:

        ```sql
        AND act.standard_value::TEXT ~ '^[0-9\.]+$'
        ```

*   **L·ªói Python (Python Error):**

    *   *English:* "Old scikit-learn version does not support `squared=False` in `mean_squared_error`"
    *   *Vietnamese:* "Phi√™n b·∫£n scikit-learn c≈© kh√¥ng h·ªó tr·ª£ tham s·ªë `squared=False` trong h√†m `mean_squared_error`"
    *   *Explanation:* The `squared=False` parameter was introduced in a later version of scikit-learn.
    *   *Fix:*  Remove the `squared=False` argument. The default behavior is to return the mean squared error (MSE). If you need the Root Mean Squared Error (RMSE), take the square root of the MSE:

        ```python
        mse = mean_squared_error(y_test, y_pred)
        rmse = np.sqrt(mse)
        print(f"Root Mean Squared Error: {rmse}")
        ```

**4. Th√™m 4 v√≠ d·ª• code .sql v√† python m·∫´u (4 More Code Examples)**

**SQL Example 2: Extracting data for a specific protein family**

```sql
-- English: Extracting data for a specific protein family (e.g., Kinases)
-- Vietnamese: Tr√≠ch xu·∫•t d·ªØ li·ªáu cho m·ªôt h·ªç protein c·ª• th·ªÉ (v√≠ d·ª•: Kinases)
SELECT DISTINCT ON (cmpd.chembl_id)
    cmpd.chembl_id,
    act.standard_value,
    act.standard_units,
    act.standard_type,
    mol.smiles
FROM
    compound_structures AS mol
    JOIN activities AS act ON mol.molregno = act.molregno
    JOIN assays AS ass ON act.assay_id = ass.assay_id
    JOIN target_dictionary AS td ON ass.tid = td.tid
    JOIN molecule_dictionary AS cmpd ON mol.molregno = cmpd.molregno
WHERE
    td.target_type = 'PROTEIN FAMILY'
    AND td.pref_name = 'Kinases'
    AND act.standard_type = 'IC50'
    AND act.standard_units = 'nM'
    AND act.standard_value IS NOT NULL
    AND act.standard_value > 0
    AND mol.smiles IS NOT NULL
    AND act.standard_value::TEXT ~ '^[0-9\.]+$'
LIMIT 100;
```

**Python Example 2:  Feature Importance from Random Forest**

```python
# English: Python code to display feature importance from the Random Forest model
# Vietnamese: M√£ Python ƒë·ªÉ hi·ªÉn th·ªã t·∫ßm quan tr·ªçng c·ªßa c√°c ƒë·∫∑c tr∆∞ng t·ª´ m√¥ h√¨nh Random Forest

import pandas as pd
from rdkit import Chem
from rdkit.Chem import Descriptors
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error, r2_score
import numpy as np
import os

# Define base path
base_path = "." # Assuming the notebook is in the root directory
data_path = os.path.join(base_path, "data")

# Load data from CSV
try:
    df = pd.read_csv(os.path.join(data_path, "chembl_ic50_data.csv")) # Adjust filename accordingly
except FileNotFoundError:
    print(f"Error: File not found at {os.path.join(data_path, 'chembl_ic50_data.csv')}")
    exit()

# Function to calculate molecular descriptors
def calculate_descriptors(smiles):
    mol = Chem.MolFromSmiles(smiles)
    if mol is None:
        return None
    descriptors = {}
    for name, func in Descriptors.descList:
        try:
            descriptors[name] = func(mol)
        except:
            descriptors[name] = None  # Handle potential errors during descriptor calculation
    return descriptors

# Apply descriptor calculation
df['descriptors'] = df['smiles'].apply(calculate_descriptors)

# Handle errors and exclude rows with None descriptors
df = df.dropna(subset=['descriptors'])

# Convert descriptors to a DataFrame
descriptors_df = pd.DataFrame(df['descriptors'].tolist())

# Check for completely empty columns
empty_cols = descriptors_df.columns[descriptors_df.isnull().all()]
descriptors_df.drop(empty_cols, axis=1, inplace=True) #Remove empty columns

# Combine descriptors with activity data
X = descriptors_df
y = df['standard_value'].astype(float)  # Convert to numeric, handle potential conversion errors

# Split data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Handle missing values using imputation (replace NaN with mean)
from sklearn.impute import SimpleImputer
imputer = SimpleImputer(strategy='mean')
X_train = imputer.fit_transform(X_train)
X_test = imputer.transform(X_test)

# Train a Random Forest model
model = RandomForestRegressor(n_estimators=100, random_state=42)
model.fit(X_train, y_train)

# Get feature importances
importances = model.feature_importances_

# Create a DataFrame to display feature importances
feature_importances = pd.DataFrame({'Feature': descriptors_df.columns, 'Importance': importances})
feature_importances = feature_importances.sort_values('Importance', ascending=False)

print(feature_importances)
```

**SQL Example 3: Extracting data based on Lipinski's Rule of Five**

```sql
-- English: Extracting compounds that satisfy Lipinski's Rule of Five
-- Vietnamese: Tr√≠ch xu·∫•t c√°c h·ª£p ch·∫•t th·ªèa m√£n Quy t·∫Øc 5 c·ªßa Lipinski
SELECT DISTINCT ON (cmpd.chembl_id)
    cmpd.chembl_id,
    act.standard_value,
    act.standard_units,
    act.standard_type,
    mol.smiles
FROM
    compound_structures AS mol
    JOIN activities AS act ON mol.molregno = act.molregno
    JOIN molecule_dictionary AS cmpd ON mol.molregno = cmpd.molregno
WHERE
    cmpd.mw_freebase <= 500  -- Molecular weight <= 500
    AND cmpd.alogp <= 5      -- LogP <= 5
    AND cmpd.psa <= 140      -- Polar Surface Area <= 140
    AND cmpd.hba <= 10       -- Hydrogen Bond Acceptors <= 10
    AND cmpd.hbd <= 5        -- Hydrogen Bond Donors <= 5
    AND act.standard_type = 'IC50'
    AND act.standard_units = 'nM'
    AND act.standard_value IS NOT NULL
    AND act.standard_value > 0
    AND mol.smiles IS NOT NULL
    AND act.standard_value::TEXT ~ '^[0-9\.]+$'
LIMIT 100;
```

**Python Example 3:  Using different Machine learning Model (SVM)**

```python
# English: Python code to train a Support Vector Machine (SVM) model
# Vietnamese: M√£ Python ƒë·ªÉ hu·∫•n luy·ªán m√¥ h√¨nh Support Vector Machine (SVM)

import pandas as pd
from rdkit import Chem
from rdkit.Chem import Descriptors
from sklearn.model_selection import train_test_split
from sklearn.svm import SVR
from sklearn.metrics import mean_squared_error, r2_score
import numpy as np
import os

# Define base path
base_path = "." # Assuming the notebook is in the root directory
data_path = os.path.join(base_path, "data")

# Load data from CSV
try:
    df = pd.read_csv(os.path.join(data_path, "chembl_ic50_data.csv")) # Adjust filename accordingly
except FileNotFoundError:
    print(f"Error: File not found at {os.path.join(data_path, 'chembl_ic50_data.csv')}")
    exit()

# Function to calculate molecular descriptors
def calculate_descriptors(smiles):
    mol = Chem.MolFromSmiles(smiles)
    if mol is None:
        return None
    descriptors = {}
    for name, func in Descriptors.descList:
        try:
            descriptors[name] = func(mol)
        except:
            descriptors[name] = None  # Handle potential errors during descriptor calculation
    return descriptors

# Apply descriptor calculation
df['descriptors'] = df['smiles'].apply(calculate_descriptors)

# Handle errors and exclude rows with None descriptors
df = df.dropna(subset=['descriptors'])

# Convert descriptors to a DataFrame
descriptors_df = pd.DataFrame(df['descriptors'].tolist())

# Check for completely empty columns
empty_cols = descriptors_df.columns[descriptors_df.isnull().all()]
descriptors_df.drop(empty_cols, axis=1, inplace=True) #Remove empty columns

# Combine descriptors with activity data
X = descriptors_df
y = df['standard_value'].astype(float)  # Convert to numeric, handle potential conversion errors

# Split data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Handle missing values using imputation (replace NaN with mean)
from sklearn.impute import SimpleImputer
imputer = SimpleImputer(strategy='mean')
X_train = imputer.fit_transform(X_train)
X_test = imputer.transform(X_test)

# Train an SVM model
model = SVR(kernel='rbf')  # You can experiment with different kernels (e.g., 'linear', 'poly')
model.fit(X_train, y_train)

# Make predictions
y_pred = model.predict(X_test)

# Evaluate the model
mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)

print(f"Mean Squared Error: {mse}")
print(f"R-squared: {r2}")
```

**SQL Example 4: Using a specific assay type**

```sql
-- English: Extracting data for a specific assay type (e.g., 'Binding')
-- Vietnamese: Tr√≠ch xu·∫•t d·ªØ li·ªáu cho m·ªôt lo·∫°i assay c·ª• th·ªÉ (v√≠ d·ª•: 'Binding')
SELECT DISTINCT ON (cmpd.chembl_id)
    cmpd.chembl_id,
    act.standard_value,
    act.standard_units,
    act.standard_type,
    mol.smiles
FROM
    compound_structures AS mol
    JOIN activities AS act ON mol.molregno = act.molregno
    JOIN assays AS ass ON act.assay_id = ass.assay_id
    JOIN target_dictionary AS td ON ass.tid = td.tid
    JOIN molecule_dictionary AS cmpd ON mol.molregno = cmpd.molregno
WHERE
    ass.assay_type = 'B'
    AND act.standard_type = 'Ki'
    AND act.standard_units = 'nM'
    AND act.standard_value IS NOT NULL
    AND act.standard_value > 0
    AND mol.smiles IS NOT NULL
    AND act.standard_value::TEXT ~ '^[0-9\.]+$'
LIMIT 100;
```

**Python Example 4:  Scaling Descriptor with StandardScaler**

```python
# English: Python code StandardScaler scaling for descriptor data
# Vietnamese: M√£ Python chu·∫©n h√≥a StandardScaler cho d·ªØ li·ªáu descriptor

import pandas as pd
from rdkit import Chem
from rdkit.Chem import Descriptors
from sklearn.model_selection import train_test_split
from sklearn.svm import SVR
from sklearn.metrics import mean_squared_error, r2_score
import numpy as np
import os

# Define base path
base_path = "." # Assuming the notebook is in the root directory
data_path = os.path.join(base_path, "data")

# Load data from CSV
try:
    df = pd.read_csv(os.path.join(data_path, "chembl_ic50_data.csv")) # Adjust filename accordingly
except FileNotFoundError:
    print(f"Error: File not found at {os.path.join(data_path, 'chembl_ic50_data.csv')}")
    exit()

# Function to calculate molecular descriptors
def calculate_descriptors(smiles):
    mol = Chem.MolFromSmiles(smiles)
    if mol is None:
        return None
    descriptors = {}
    for name, func in Descriptors.descList:
        try:
            descriptors[name] = func(mol)
        except:
            descriptors[name] = None  # Handle potential errors during descriptor calculation
    return descriptors

# Apply descriptor calculation
df['descriptors'] = df['smiles'].apply(calculate_descriptors)

# Handle errors and exclude rows with None descriptors
df = df.dropna(subset=['descriptors'])

# Convert descriptors to a DataFrame
descriptors_df = pd.DataFrame(df['descriptors'].tolist())

# Check for completely empty columns
empty_cols = descriptors_df.columns[descriptors_df.isnull().all()]
descriptors_df.drop(empty_cols, axis=1, inplace=True) #Remove empty columns

# Combine descriptors with activity data
X = descriptors_df
y = df['standard_value'].astype(float)  # Convert to numeric, handle potential conversion errors

# Split data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Handle missing values using imputation (replace NaN with mean)
from sklearn.impute import SimpleImputer
imputer = SimpleImputer(strategy='mean')
X_train = imputer.fit_transform(X_train)
X_test = imputer.transform(X_test)

# Standardize the feature with StandardScaler
from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)


# Train an SVM model
model = SVR(kernel='rbf')  # You can experiment with different kernels (e.g., 'linear', 'poly')
model.fit(X_train_scaled, y_train)

# Make predictions
y_pred = model.predict(X_test_scaled)

# Evaluate the model
mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)

print(f"Mean Squared Error: {mse}")
print(f"R-squared: {r2}")
```

**L∆∞u √Ω quan tr·ªçng (Important Notes):**

*   **ƒê∆∞·ªùng d·∫´n (Paths):**  ƒê·∫£m b·∫£o r·∫±ng ƒë∆∞·ªùng d·∫´n ƒë·∫øn file CSV c·ªßa b·∫°n ch√≠nh x√°c. S·ª≠ d·ª•ng `os.path.join(base_path, "data", "your_file.csv")` ƒë·ªÉ ƒë·∫£m b·∫£o t√≠nh di ƒë·ªông c·ªßa code.
*   **X·ª≠ l√Ω l·ªói (Error Handling):**  Th√™m c√°c kh·ªëi `try...except` ƒë·ªÉ x·ª≠ l√Ω c√°c l·ªói c√≥ th·ªÉ x·∫£y ra trong qu√° tr√¨nh t√≠nh to√°n descriptor ho·∫∑c ƒë·ªçc file.
*   **Phi√™n b·∫£n th∆∞ vi·ªán (Library Versions):**  ƒê·∫£m b·∫£o r·∫±ng b·∫°n ƒëang s·ª≠ d·ª•ng c√°c phi√™n b·∫£n th∆∞ vi·ªán t∆∞∆°ng th√≠ch. N·∫øu b·∫°n g·∫∑p l·ªói li√™n quan ƒë·∫øn `squared=False`, h√£y n√¢ng c·∫•p scikit-learn ho·∫∑c lo·∫°i b·ªè tham s·ªë ƒë√≥.
*   **L·ªçc d·ªØ li·ªáu (Data Filtering):** L·ªçc d·ªØ li·ªáu c·∫©n th·∫≠n ƒë·ªÉ ƒë·∫£m b·∫£o r·∫±ng b·∫°n ƒëang l√†m vi·ªác v·ªõi d·ªØ li·ªáu ch·∫•t l∆∞·ª£ng cao.  Lo·∫°i b·ªè c√°c h·ª£p ch·∫•t c√≥ SMILES kh√¥ng h·ª£p l·ªá ho·∫∑c c√°c gi√° tr·ªã ho·∫°t t√≠nh kh√¥ng ƒë√°ng tin c·∫≠y.
*   **Tu·ª≥ ch·ªânh m√¥ h√¨nh (Model Tuning):** Th·ª≠ nghi·ªám v·ªõi c√°c thu·∫≠t to√°n h·ªçc m√°y kh√°c nhau v√† c√°c tham s·ªë c·ªßa ch√∫ng ƒë·ªÉ t√¨m ra m√¥ h√¨nh t·ªët nh·∫•t cho d·ªØ li·ªáu c·ªßa b·∫°n.  S·ª≠ d·ª•ng cross-validation ƒë·ªÉ ƒë√°nh gi√° hi·ªáu su·∫•t c·ªßa m√¥ h√¨nh m·ªôt c√°ch kh√°ch quan.
*   **L∆∞u tr·ªØ k·∫øt qu·∫£ (Result saving):** L∆∞u tr·ªØ c√°c m√¥ h√¨nh, descriptor ƒë√£ t√≠nh to√°n ƒë·ªÉ c√≥ th·ªÉ t√°i s·ª≠ d·ª•ng.

Ch√∫c b·∫°n th√†nh c√¥ng v·ªõi d·ª± √°n c·ªßa m√¨nh! H√£y cho t√¥i bi·∫øt n·∫øu b·∫°n c√≥ b·∫•t k·ª≥ c√¢u h·ªèi n√†o kh√°c.